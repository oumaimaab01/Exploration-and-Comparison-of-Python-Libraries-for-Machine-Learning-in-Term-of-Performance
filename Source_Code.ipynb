{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1623d083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd706e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\python\\python311\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\python\\python311\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e9e0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\python\\python311\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: wheel in c:\\python\\python311\\lib\\site-packages (from lightgbm) (0.40.0)\n",
      "Requirement already satisfied: numpy in c:\\python\\python311\\lib\\site-packages (from lightgbm) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\python\\python311\\lib\\site-packages (from lightgbm) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\python\\python311\\lib\\site-packages (from lightgbm) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python\\python311\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python\\python311\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ef3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\python\\python311\\lib\\site-packages (2.0.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\python\\python311\\lib\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\python\\python311\\lib\\site-packages (from torch) (4.6.2)\n",
      "Requirement already satisfied: sympy in c:\\python\\python311\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python\\python311\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\python311\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python311\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python\\python311\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56308e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\python\\python311\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c51a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp311-cp311-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 86.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------- 298.0/298.0 kB 79.0 kB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting catboost\n",
      "  Downloading catboost-1.2-cp311-cp311-win_amd64.whl (101.0 MB)\n",
      "     ------------------------------------ 101.0/101.0 MB 131.1 kB/s eta 0:00:00\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "     -------------------------------------- 47.0/47.0 kB 391.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from catboost) (3.6.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from catboost) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from catboost) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from catboost) (5.11.0)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=0.24->catboost) (2022.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->catboost) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->catboost) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->catboost) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hp\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from plotly->catboost) (8.1.0)\n",
      "Installing collected packages: graphviz, catboost\n",
      "Successfully installed catboost-1.2 graphviz-0.20.1\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8206392",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASSIFICATION_ALGORITHMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bedce827",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  DECISION_TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef3273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sickit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f26f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn Accuracy: 1.0\n",
      "Temps d'exécution: 0.02398991584777832 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score  \n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Code dont vous voulez mesurer le temps d'exécution\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier (max_depth=6)\n",
    "\n",
    "# Train the model\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "#decision tree\n",
    "# Measure the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"scikit-learn Accuracy:\", accuracy)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time, \"secondes\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ab1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1843e63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Temps d'exécution: 0.1325390338897705 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of XGBClassifier\n",
    "model = xgb.XGBClassifier(objective='multiclass', num_class=3,n_estimators = 100, max_depth=6, learning_rate=0.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time, \"secondes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68bb3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83f469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Accuracy: 1.0\n",
      "Temps d'exécution: 0.16683197021484375 secondes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# charger dataset\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "# diviser les données en 2 parties , une pour apprentissage et l’autre cpour le test \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Entrainement du modèle\n",
    "lgbm = LGBMClassifier(objective='multiclass', num_class=3, max_depth=6, learning_rate=0.1)\n",
    "lgbm.fit(X_train ,y_train)\n",
    "\n",
    "\n",
    "# Model Accuracy  \n",
    "lgbm_pred = lgbm.predict(X_test)\n",
    "lgbm_acc = accuracy_score(y_test, lgbm_pred)\n",
    "print(\"LightGBM Accuracy:\", lgbm_acc)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBOSST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7825bc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Temps d'exécution: 0.4122912883758545 secondes\n"
     ]
    }
   ],
   "source": [
    "import catboost as cb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Charger les données de l'ensemble de données Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Créer un modèle de classification avec Catboost\n",
    "model = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='MultiClass')\n",
    "\n",
    "\n",
    "# Entraînement du modèle \n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "\n",
    "# Prédire les  classes pour les données de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Évaluer la précision du modèle \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf863e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN_Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af0175ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sickit-Learn ANN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "513d4ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9667\n",
      "Temps d'exécution: 0.11132359504699707 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger les données de l'Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Créer le modèle de réseau de neurones\n",
    "model = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=100)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculer la précision\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943d87d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras ANN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b6a93cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 4ms/step - loss: 0.9082 - accuracy: 0.6000\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.7333\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.5492 - accuracy: 0.7583\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8667\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.4212 - accuracy: 0.8583\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3726 - accuracy: 0.8833\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3254 - accuracy: 0.9417\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3015 - accuracy: 0.9250\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2863 - accuracy: 0.9417\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2681 - accuracy: 0.9083\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9583\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.2395 - accuracy: 0.9583\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2167 - accuracy: 0.9500\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2119 - accuracy: 0.9333\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1828 - accuracy: 0.9667\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1812 - accuracy: 0.9583\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1766 - accuracy: 0.9333\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1803 - accuracy: 0.9417\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1758 - accuracy: 0.9500\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9667\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1564 - accuracy: 0.9417\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1390 - accuracy: 0.9583\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1489 - accuracy: 0.9500\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1430 - accuracy: 0.9583\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1310 - accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.9833\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1234 - accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1156 - accuracy: 0.9750\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1340 - accuracy: 0.9417\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1184 - accuracy: 0.9583\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1142 - accuracy: 0.9750\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1317 - accuracy: 0.9417\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1016 - accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1031 - accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1056 - accuracy: 0.9500\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1055 - accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1036 - accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1106 - accuracy: 0.9500\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1045 - accuracy: 0.9583\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0808 - accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0904 - accuracy: 0.9833\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.9500\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9583\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9500\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0968 - accuracy: 0.9583\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0979 - accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0901 - accuracy: 0.9750\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9583\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1049 - accuracy: 0.9583\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0943 - accuracy: 0.9750\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.9583\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0882 - accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0625 - accuracy: 0.9833\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1034 - accuracy: 0.9583\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9750\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0998 - accuracy: 0.9583\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9583\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0931 - accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9583\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0801 - accuracy: 0.9583\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0993 - accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9583\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9750\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0831 - accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0854 - accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9750\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9583\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0876 - accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9750\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9417\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0788 - accuracy: 0.9500\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9750\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9750\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9750\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0635 - accuracy: 0.9750\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9500\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9833\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9583\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9750\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0862 - accuracy: 0.9583\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0820 - accuracy: 0.9750\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9750\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9750\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0707 - accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0649 - accuracy: 0.9750\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9583\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9583\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9500\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.0608 - accuracy: 1.0000\n",
      "Test Accuracy:  1.0\n",
      "Temps d'exécution: 11.464237928390503 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Create the network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(4,)))\n",
    "network.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the network\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Load the iris dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create training and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Create categorical labels\n",
    "\n",
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)\n",
    "\n",
    "# Fit the neural network\n",
    "\n",
    "network.fit(X_train, train_labels, epochs=100, batch_size=8)\n",
    "\n",
    "# Get the accuracy of test data set\n",
    "test_loss, test_acc = network.evaluate(X_test, test_labels)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy: ',test_acc)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f895f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensoFlow ANN Classifier using Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8708b9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 2s 29ms/step - loss: 1.5086 - accuracy: 0.2750 - val_loss: 1.0119 - val_accuracy: 0.6667\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.0280 - accuracy: 0.6417 - val_loss: 0.9020 - val_accuracy: 0.7000\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.8856 - accuracy: 0.6250 - val_loss: 0.8249 - val_accuracy: 0.8667\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.7732 - accuracy: 0.7750 - val_loss: 0.7125 - val_accuracy: 0.7000\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.7333 - val_loss: 0.6211 - val_accuracy: 0.7333\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6086 - accuracy: 0.9000 - val_loss: 0.5544 - val_accuracy: 0.8000\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.5379 - accuracy: 0.8667 - val_loss: 0.4861 - val_accuracy: 0.8000\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.4778 - accuracy: 0.8250 - val_loss: 0.4437 - val_accuracy: 0.9667\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.4317 - accuracy: 0.9667 - val_loss: 0.4098 - val_accuracy: 0.9333\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.4019 - accuracy: 0.9417 - val_loss: 0.3781 - val_accuracy: 0.8333\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3642 - accuracy: 0.9417 - val_loss: 0.3572 - val_accuracy: 0.9000\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.3448 - accuracy: 0.9583 - val_loss: 0.3282 - val_accuracy: 0.9667\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.3176 - accuracy: 0.9500 - val_loss: 0.3107 - val_accuracy: 0.9333\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2988 - accuracy: 0.9583 - val_loss: 0.2870 - val_accuracy: 0.9667\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2869 - accuracy: 0.9500 - val_loss: 0.2800 - val_accuracy: 0.9667\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.3046 - accuracy: 0.8917 - val_loss: 0.3145 - val_accuracy: 0.8667\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2611 - accuracy: 0.9583 - val_loss: 0.2372 - val_accuracy: 0.9667\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2252 - accuracy: 0.9750 - val_loss: 0.2240 - val_accuracy: 0.9667\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.2071 - accuracy: 0.9833 - val_loss: 0.2146 - val_accuracy: 0.9667\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1936 - accuracy: 0.9833 - val_loss: 0.2008 - val_accuracy: 0.9667\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1852 - accuracy: 0.9667 - val_loss: 0.2031 - val_accuracy: 0.8667\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1815 - accuracy: 0.9583 - val_loss: 0.1945 - val_accuracy: 0.8667\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1610 - accuracy: 0.9667 - val_loss: 0.1701 - val_accuracy: 0.9667\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1555 - accuracy: 0.9750 - val_loss: 0.1670 - val_accuracy: 0.9333\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1472 - accuracy: 0.9583 - val_loss: 0.1557 - val_accuracy: 0.9667\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1346 - accuracy: 0.9750 - val_loss: 0.1511 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1423 - accuracy: 0.9583 - val_loss: 0.1538 - val_accuracy: 0.9667\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1362 - accuracy: 0.9583 - val_loss: 0.1405 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1575 - accuracy: 0.9417 - val_loss: 0.1633 - val_accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1205 - accuracy: 0.9750 - val_loss: 0.1360 - val_accuracy: 0.9667\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1395 - accuracy: 0.9333 - val_loss: 0.1709 - val_accuracy: 0.9667\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.1158 - accuracy: 0.9833 - val_loss: 0.1656 - val_accuracy: 0.9000\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1043 - accuracy: 0.9833 - val_loss: 0.1227 - val_accuracy: 0.9667\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.1103 - accuracy: 0.9667 - val_loss: 0.1289 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1145 - accuracy: 0.9667 - val_loss: 0.1364 - val_accuracy: 0.8667\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1171 - accuracy: 0.9667 - val_loss: 0.1174 - val_accuracy: 0.9667\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0995 - accuracy: 0.9667 - val_loss: 0.1145 - val_accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0993 - accuracy: 0.9750 - val_loss: 0.1140 - val_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0893 - accuracy: 0.9833 - val_loss: 0.1177 - val_accuracy: 0.9667\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0908 - accuracy: 0.9750 - val_loss: 0.1097 - val_accuracy: 0.9667\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0981 - accuracy: 0.9583 - val_loss: 0.1196 - val_accuracy: 0.9333\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0984 - accuracy: 0.9750 - val_loss: 0.1154 - val_accuracy: 0.9333\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0838 - accuracy: 0.9750 - val_loss: 0.1038 - val_accuracy: 0.9667\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0879 - accuracy: 0.9750 - val_loss: 0.1025 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0976 - accuracy: 0.9667 - val_loss: 0.1486 - val_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0891 - accuracy: 0.9667 - val_loss: 0.1005 - val_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0919 - accuracy: 0.9667 - val_loss: 0.1575 - val_accuracy: 0.9000\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0795 - accuracy: 0.9750 - val_loss: 0.1465 - val_accuracy: 0.9667\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0956 - accuracy: 0.9583 - val_loss: 0.1151 - val_accuracy: 0.9000\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9750 - val_loss: 0.1048 - val_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9833 - val_loss: 0.1024 - val_accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0857 - accuracy: 0.9583 - val_loss: 0.0976 - val_accuracy: 0.9667\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.1109 - accuracy: 0.9583 - val_loss: 0.1506 - val_accuracy: 0.9000\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0795 - accuracy: 0.9667 - val_loss: 0.0930 - val_accuracy: 0.9667\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0824 - accuracy: 0.9750 - val_loss: 0.1779 - val_accuracy: 0.9000\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0865 - accuracy: 0.9583 - val_loss: 0.0924 - val_accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0893 - accuracy: 0.9667 - val_loss: 0.0902 - val_accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0804 - accuracy: 0.9833 - val_loss: 0.1092 - val_accuracy: 0.9000\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0784 - accuracy: 0.9750 - val_loss: 0.0890 - val_accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0770 - accuracy: 0.9750 - val_loss: 0.1360 - val_accuracy: 0.9000\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0703 - accuracy: 0.9667 - val_loss: 0.0894 - val_accuracy: 0.9667\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0860 - accuracy: 0.9750 - val_loss: 0.0963 - val_accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0796 - accuracy: 0.9667 - val_loss: 0.0866 - val_accuracy: 0.9667\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0737 - accuracy: 0.9750 - val_loss: 0.0918 - val_accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0752 - accuracy: 0.9667 - val_loss: 0.0856 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0758 - accuracy: 0.9583 - val_loss: 0.0849 - val_accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0740 - accuracy: 0.9750 - val_loss: 0.0855 - val_accuracy: 0.9667\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0730 - accuracy: 0.9750 - val_loss: 0.0857 - val_accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0710 - accuracy: 0.9750 - val_loss: 0.0862 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0721 - accuracy: 0.9750 - val_loss: 0.0984 - val_accuracy: 0.9333\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0754 - accuracy: 0.9667 - val_loss: 0.1041 - val_accuracy: 0.9333\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0730 - accuracy: 0.9667 - val_loss: 0.0886 - val_accuracy: 0.9667\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0721 - accuracy: 0.9667 - val_loss: 0.0831 - val_accuracy: 0.9667\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0873 - accuracy: 0.9667 - val_loss: 0.1020 - val_accuracy: 0.9333\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.0896 - accuracy: 0.9583 - val_loss: 0.1180 - val_accuracy: 0.9000\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9667 - val_loss: 0.0819 - val_accuracy: 0.9667\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0716 - accuracy: 0.9667 - val_loss: 0.0884 - val_accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0651 - accuracy: 0.9750 - val_loss: 0.0800 - val_accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0713 - accuracy: 0.9667 - val_loss: 0.0856 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0704 - accuracy: 0.9833 - val_loss: 0.1230 - val_accuracy: 0.9000\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0819 - accuracy: 0.9583 - val_loss: 0.0858 - val_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0729 - accuracy: 0.9750 - val_loss: 0.1117 - val_accuracy: 0.9333\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0899 - accuracy: 0.9667 - val_loss: 0.0882 - val_accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0944 - accuracy: 0.9583 - val_loss: 0.0828 - val_accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0750 - accuracy: 0.9750 - val_loss: 0.0792 - val_accuracy: 0.9667\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0805 - accuracy: 0.9583 - val_loss: 0.0783 - val_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0712 - accuracy: 0.9750 - val_loss: 0.0903 - val_accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0656 - accuracy: 0.9667 - val_loss: 0.0784 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0730 - accuracy: 0.9667 - val_loss: 0.1021 - val_accuracy: 0.9333\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0761 - accuracy: 0.9667 - val_loss: 0.0772 - val_accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0739 - accuracy: 0.9583 - val_loss: 0.0766 - val_accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0905 - accuracy: 0.9583 - val_loss: 0.0824 - val_accuracy: 0.9667\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0827 - accuracy: 0.9667 - val_loss: 0.1118 - val_accuracy: 0.9000\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 0.0789 - accuracy: 0.9583 - val_loss: 0.0830 - val_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0857 - accuracy: 0.9667 - val_loss: 0.0875 - val_accuracy: 0.9667\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0625 - accuracy: 0.9750 - val_loss: 0.0780 - val_accuracy: 0.9667\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0678 - accuracy: 0.9750 - val_loss: 0.0935 - val_accuracy: 0.9667\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0654 - accuracy: 0.9750 - val_loss: 0.0744 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.0943 - accuracy: 0.9667 - val_loss: 0.0960 - val_accuracy: 0.9333\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.0910 - accuracy: 0.9583 - val_loss: 0.0773 - val_accuracy: 0.9667\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.0773 - accuracy: 0.9667\n",
      "Test accuracy: 0.9666666388511658\n",
      "Temps d'exécution: 13.039796829223633 secondes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model\n",
    "model = models.Sequential ([\n",
    "  layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3636f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow ANN Classifier Low-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c0ecb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93333334\n",
      "Temps d'exécution: 5.914903402328491 secondes\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger les données de l'Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Convertir les étiquettes en nombres entiers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convertir les étiquettes en int64\n",
    "y_train = y_train.astype('int64')\n",
    "y_test = y_test.astype('int64')\n",
    "\n",
    "\n",
    "# Définir les paramètres du modèle\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "batch_size = 8\n",
    "\n",
    "# Définir les tenseurs de données d'entraînement et de test\n",
    "X_train_tensor = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "y_train_tensor = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
    "X_test_tensor = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
    "y_test_tensor = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
    "\n",
    "# Créer les tenseurs pour les poids et les biais\n",
    "weights1 = tf.Variable(tf.random.normal([input_size, hidden_size]))\n",
    "bias1 = tf.Variable(tf.random.normal([hidden_size]))\n",
    "weights2 = tf.Variable(tf.random.normal([hidden_size, output_size]))\n",
    "bias2 = tf.Variable(tf.random.normal([output_size]))\n",
    "\n",
    "# Définir la fonction d'activation ReLU\n",
    "def relu(x):\n",
    "    return tf.maximum(0, x)\n",
    "\n",
    "# Définir la fonction softmax pour la couche de sortie\n",
    "def softmax(x):\n",
    "    exp_vals = tf.exp(x)\n",
    "    return exp_vals / tf.reduce_sum(exp_vals, axis=1, keepdims=True)\n",
    "\n",
    "# Définir la boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    # Mélanger les indices des données d'entraînement\n",
    "    indices = tf.random.shuffle(tf.range(len(X_train)))\n",
    "\n",
    "    for start_index in range(0, len(X_train), batch_size):\n",
    "        # Sélectionner un lot d'indices\n",
    "        batch_indices = indices[start_index:start_index + batch_size]\n",
    "\n",
    "        # Obtenir les lots d'entrée et de sortie correspondants\n",
    "        X_batch = tf.gather(X_train_tensor, batch_indices)\n",
    "        y_batch = tf.gather(y_train_tensor, batch_indices)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculer les sorties de la première couche cachée\n",
    "            hidden_output = relu(tf.matmul(X_batch, weights1) + bias1)\n",
    "            \n",
    "            # Calculer les sorties de la couche de sortie\n",
    "            output_logits = tf.matmul(hidden_output, weights2) + bias2\n",
    "            output_probs = softmax(output_logits)\n",
    "            \n",
    "            # Calculer la perte en utilisant la fonction d'entropie croisée\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_batch, logits=output_logits))\n",
    "\n",
    "        # Calculer les gradients\n",
    "        gradients = tape.gradient(loss, [weights1, bias1, weights2, bias2])\n",
    "\n",
    "        # Mettre à jour les poids et les biais\n",
    "        weights1.assign_sub(learning_rate * gradients[0])\n",
    "        bias1.assign_sub(learning_rate * gradients[1])\n",
    "        weights2.assign_sub(learning_rate * gradients[2])\n",
    "        bias2.assign_sub(learning_rate * gradients[3])\n",
    "\n",
    "# Prédire les étiquettes pour l'ensemble de test\n",
    "hidden_output_test = relu(tf.matmul(X_test_tensor, weights1) + bias1)\n",
    "output_logits_test = tf.matmul(hidden_output_test, weights2) + bias2\n",
    "output_probs_test = softmax(output_logits_test)\n",
    "predicted_labels = tf.argmax(output_probs_test, axis=1)\n",
    "\n",
    "# Calculer la précision\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted_labels, tf.cast(y_test_tensor, dtype=tf.int64)), dtype=tf.float32))\n",
    "# Afficher la précision\n",
    "print(\"Accuracy:\", accuracy.numpy())\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c69effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ANN Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37e13490",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3286de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9667\n",
      "Temps d'exécution: 3.815518856048584 secondes\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Charger les données de l'Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Convertir les étiquettes en nombres entiers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir les données en tenseurs PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Créer un DataLoader pour l'ensemble d'entraînement\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Définir le modèle de réseau de neurones\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Créer une instance du modèle\n",
    "model = NeuralNet()\n",
    "\n",
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraîner le modèle\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "53158df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### REGRESSION_ALGORITHMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e984cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af7b33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression Sickit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "035203ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.555891598695242\n",
      "Temps d'exécution: 0.032340049743652344 secondes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908923d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f43a9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6335500950429376\n",
      "Temps d'exécution: 0.19997477531433105 secondes\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger la dataset California Housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create an instance of XGBRegressor with linear base learner\n",
    "model = xgb.XGBRegressor(booster=\"gblinear\")\n",
    "\n",
    "# Entraîner le modèle de régression XGBoost\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Effectuer des prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b343d55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8373feef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "516/516 [==============================] - 2s 2ms/step - loss: 5.1937\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 3.1690\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 2.0621\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.4411\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.1109\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.9453\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.8598\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.8111\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.7740\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.7411\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.7121\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6849\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6590\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6371\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6178\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6012\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5847\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5707\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5597\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5503\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5422\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5355\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5296\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5256\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5225\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5207\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5199\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5186\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5197\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5197\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5186\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5196\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5187\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5196\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5185\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5187\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5187\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Mean Squared Error (MSE): 0.5553398728370667\n",
      "le temps d'exécution est : 104.080575466156 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Chargement des données California Housing\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Normalisation des caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fractionnement des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(X_train.shape[1],), activation='linear'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error (MSE): {loss}\")\n",
    "\n",
    "end=time.time()\n",
    "t=end - start\n",
    "print(\"le temps d'exécution est :\",t,\"secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7c03889",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression TensorFlow using Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "64758460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "516/516 [==============================] - 2s 2ms/step - loss: 4.9478\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 3.0182\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.9072\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.2615\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.9086\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.7321\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6478\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.6063\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5811\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5637\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5515\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5427\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5364\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5317\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5280\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5258\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5240\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5227\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5217\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5209\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5205\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5203\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5200\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5199\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5197\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5199\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5196\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5195\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5188\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5187\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5192\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5193\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5191\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5194\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5189\n",
      "129/129 [==============================] - 0s 2ms/step\n",
      "Mean Squared Error: 0.5575329661369324\n",
      "le temps d'exécution est de :   105.307936668396 Seconds \n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start=time.time()\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(X_train.shape[1],))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "end=time.time()\n",
    "t=end - start\n",
    "print(\"le temps d'exécution est de :  \",t,\"Seconds \") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d11db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow low level Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9a2ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Epoch 1 Loss: 13.017645299896714\n",
      "Epoch 2 Loss: 3.745787449585375\n",
      "Epoch 3 Loss: 1.8563633073092431\n",
      "Epoch 4 Loss: 1.45477499495181\n",
      "Epoch 5 Loss: 1.3586005114538724\n",
      "Epoch 6 Loss: 1.3436679281583128\n",
      "Epoch 7 Loss: 1.3372299330410107\n",
      "Epoch 8 Loss: 1.3374347542376481\n",
      "Epoch 9 Loss: 1.3366995869680893\n",
      "Epoch 10 Loss: 1.3369131137927373\n",
      "Epoch 11 Loss: 1.336785101728846\n",
      "Epoch 12 Loss: 1.3368405959633893\n",
      "Epoch 13 Loss: 1.3368148312781207\n",
      "Epoch 14 Loss: 1.336827624329301\n",
      "Epoch 15 Loss: 1.3368223265622012\n",
      "Epoch 16 Loss: 1.3368252649787784\n",
      "Epoch 17 Loss: 1.336824209017809\n",
      "Epoch 18 Loss: 1.3368248878292335\n",
      "Epoch 19 Loss: 1.336824690302213\n",
      "Epoch 20 Loss: 1.336824858951014\n",
      "Epoch 21 Loss: 1.3368248394293378\n",
      "Epoch 22 Loss: 1.3368248754693557\n",
      "Epoch 23 Loss: 1.3368248754693557\n",
      "Epoch 24 Loss: 1.3368248784726904\n",
      "Epoch 25 Loss: 1.3368248820535897\n",
      "Epoch 26 Loss: 1.3368248827466669\n",
      "Epoch 27 Loss: 1.3368248886378236\n",
      "Epoch 28 Loss: 1.3368248907170555\n",
      "Epoch 29 Loss: 1.3368248947022496\n",
      "Epoch 30 Loss: 1.3368248879447464\n",
      "Epoch 31 Loss: 1.336824893027313\n",
      "Epoch 32 Loss: 1.3368248946444934\n",
      "Epoch 33 Loss: 1.336824896030648\n",
      "Epoch 34 Loss: 1.336824896030648\n",
      "Epoch 35 Loss: 1.336824897878854\n",
      "Epoch 36 Loss: 1.3368248961461606\n",
      "Epoch 37 Loss: 1.3368248955685964\n",
      "Epoch 38 Loss: 1.3368248952220576\n",
      "Epoch 39 Loss: 1.3368248945289805\n",
      "Epoch 40 Loss: 1.336824895799622\n",
      "Epoch 41 Loss: 1.3368248964926994\n",
      "Epoch 42 Loss: 1.3368248966082121\n",
      "Epoch 43 Loss: 1.3368248953375705\n",
      "Epoch 44 Loss: 1.336824896723725\n",
      "Epoch 45 Loss: 1.3368248954530835\n",
      "Epoch 46 Loss: 1.3368248974168024\n",
      "Epoch 47 Loss: 1.3368248966082121\n",
      "Epoch 48 Loss: 1.3368248953375705\n",
      "Epoch 49 Loss: 1.336824896723725\n",
      "Epoch 50 Loss: 1.3368248954530835\n",
      "Epoch 51 Loss: 1.3368248974168024\n",
      "Epoch 52 Loss: 1.3368248966082121\n",
      "Epoch 53 Loss: 1.3368248953375705\n",
      "Epoch 54 Loss: 1.336824896723725\n",
      "Epoch 55 Loss: 1.3368248954530835\n",
      "Epoch 56 Loss: 1.3368248974168024\n",
      "Epoch 57 Loss: 1.3368248966082121\n",
      "Epoch 58 Loss: 1.3368248953375705\n",
      "Epoch 59 Loss: 1.336824896723725\n",
      "Epoch 60 Loss: 1.3368248954530835\n",
      "Epoch 61 Loss: 1.3368248974168024\n",
      "Epoch 62 Loss: 1.3368248966082121\n",
      "Epoch 63 Loss: 1.3368248953375705\n",
      "Epoch 64 Loss: 1.336824896723725\n",
      "Epoch 65 Loss: 1.3368248954530835\n",
      "Epoch 66 Loss: 1.3368248974168024\n",
      "Epoch 67 Loss: 1.3368248966082121\n",
      "Epoch 68 Loss: 1.3368248953375705\n",
      "Epoch 69 Loss: 1.336824896723725\n",
      "Epoch 70 Loss: 1.3368248954530835\n",
      "Epoch 71 Loss: 1.3368248974168024\n",
      "Epoch 72 Loss: 1.3368248966082121\n",
      "Epoch 73 Loss: 1.3368248953375705\n",
      "Epoch 74 Loss: 1.336824896723725\n",
      "Epoch 75 Loss: 1.3368248954530835\n",
      "Epoch 76 Loss: 1.3368248974168024\n",
      "Epoch 77 Loss: 1.3368248966082121\n",
      "Epoch 78 Loss: 1.3368248953375705\n",
      "Epoch 79 Loss: 1.336824896723725\n",
      "Epoch 80 Loss: 1.3368248954530835\n",
      "Epoch 81 Loss: 1.3368248974168024\n",
      "Epoch 82 Loss: 1.3368248966082121\n",
      "Epoch 83 Loss: 1.3368248953375705\n",
      "Epoch 84 Loss: 1.336824896723725\n",
      "Epoch 85 Loss: 1.3368248954530835\n",
      "Epoch 86 Loss: 1.3368248974168024\n",
      "Epoch 87 Loss: 1.3368248966082121\n",
      "Epoch 88 Loss: 1.3368248953375705\n",
      "Epoch 89 Loss: 1.336824896723725\n",
      "Epoch 90 Loss: 1.3368248954530835\n",
      "Epoch 91 Loss: 1.3368248974168024\n",
      "Epoch 92 Loss: 1.3368248966082121\n",
      "Epoch 93 Loss: 1.3368248953375705\n",
      "Epoch 94 Loss: 1.336824896723725\n",
      "Epoch 95 Loss: 1.3368248954530835\n",
      "Epoch 96 Loss: 1.3368248974168024\n",
      "Epoch 97 Loss: 1.3368248966082121\n",
      "Epoch 98 Loss: 1.3368248953375705\n",
      "Epoch 99 Loss: 1.336824896723725\n",
      "Epoch 100 Loss: 1.3368248954530835\n",
      "Mean Squared Error: 1.2509139329494885\n",
      "Le temps d'exécution est de 40.479857444763184 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define placeholders for input and target\n",
    "X_placeholder = tf.placeholder(tf.float32, shape=(None, X_train_scaled.shape[1]))\n",
    "y_placeholder = tf.placeholder(tf.float32, shape=(None,))\n",
    "\n",
    "# Define variables for weights and bias\n",
    "weights = tf.Variable(tf.random_normal([X_train_scaled.shape[1], 1]))\n",
    "bias = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "# Define the linear regression model\n",
    "y_pred = tf.matmul(X_placeholder, weights) + bias\n",
    "\n",
    "# Define the loss function (mean squared error)\n",
    "loss = tf.reduce_mean(tf.square(y_pred - y_placeholder))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "# Create a session and initialize variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Train the model\n",
    "    num_epoch = 100\n",
    "    batch_size = 32\n",
    "    num_batches = len(X_train_scaled) // batch_size\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_loss = 0\n",
    "        for batch in range(num_batches):\n",
    "            batch_X = X_train_scaled[batch*batch_size : (batch+1)*batch_size]\n",
    "            batch_y = y_train[batch*batch_size : (batch+1)*batch_size]\n",
    "            _, batch_loss = sess.run([train_op, loss], feed_dict={X_placeholder: batch_X, y_placeholder: batch_y})\n",
    "            epoch_loss += batch_loss\n",
    "\n",
    "        print(\"Epoch\", epoch+1, \"Loss:\", epoch_loss/num_batches)\n",
    "\n",
    "    # Evaluate the model\n",
    "    y_pred_test = sess.run(y_pred, feed_dict={X_placeholder: X_test_scaled})\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "t = end - start\n",
    "print(\"Le temps d'exécution est de\", t, \"secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "78aa776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47427b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7280\n",
      "Epoch [2/100], Loss: 0.1703\n",
      "Epoch [3/100], Loss: 0.9742\n",
      "Epoch [4/100], Loss: 0.2759\n",
      "Epoch [5/100], Loss: 0.4638\n",
      "Epoch [6/100], Loss: 0.4223\n",
      "Epoch [7/100], Loss: 0.1691\n",
      "Epoch [8/100], Loss: 0.2634\n",
      "Epoch [9/100], Loss: 0.4662\n",
      "Epoch [10/100], Loss: 0.4307\n",
      "Epoch [11/100], Loss: 0.3172\n",
      "Epoch [12/100], Loss: 1.1528\n",
      "Epoch [13/100], Loss: 0.4699\n",
      "Epoch [14/100], Loss: 0.5317\n",
      "Epoch [15/100], Loss: 0.4260\n",
      "Epoch [16/100], Loss: 0.5044\n",
      "Epoch [17/100], Loss: 7.5153\n",
      "Epoch [18/100], Loss: 0.6040\n",
      "Epoch [19/100], Loss: 0.5783\n",
      "Epoch [20/100], Loss: 1.2921\n",
      "Epoch [21/100], Loss: 0.2897\n",
      "Epoch [22/100], Loss: 0.5371\n",
      "Epoch [23/100], Loss: 0.4750\n",
      "Epoch [24/100], Loss: 0.3519\n",
      "Epoch [25/100], Loss: 0.3035\n",
      "Epoch [26/100], Loss: 0.3410\n",
      "Epoch [27/100], Loss: 0.4164\n",
      "Epoch [28/100], Loss: 1.2862\n",
      "Epoch [29/100], Loss: 0.4020\n",
      "Epoch [30/100], Loss: 0.6815\n",
      "Epoch [31/100], Loss: 0.2152\n",
      "Epoch [32/100], Loss: 0.3354\n",
      "Epoch [33/100], Loss: 0.3054\n",
      "Epoch [34/100], Loss: 0.7035\n",
      "Epoch [35/100], Loss: 0.1802\n",
      "Epoch [36/100], Loss: 0.4184\n",
      "Epoch [37/100], Loss: 0.6417\n",
      "Epoch [38/100], Loss: 0.3954\n",
      "Epoch [39/100], Loss: 0.5672\n",
      "Epoch [40/100], Loss: 0.3805\n",
      "Epoch [41/100], Loss: 0.5074\n",
      "Epoch [42/100], Loss: 0.9783\n",
      "Epoch [43/100], Loss: 0.6125\n",
      "Epoch [44/100], Loss: 0.7134\n",
      "Epoch [45/100], Loss: 0.2535\n",
      "Epoch [46/100], Loss: 0.4915\n",
      "Epoch [47/100], Loss: 0.5928\n",
      "Epoch [48/100], Loss: 0.2244\n",
      "Epoch [49/100], Loss: 0.8240\n",
      "Epoch [50/100], Loss: 0.5203\n",
      "Epoch [51/100], Loss: 0.3267\n",
      "Epoch [52/100], Loss: 4.8425\n",
      "Epoch [53/100], Loss: 0.3156\n",
      "Epoch [54/100], Loss: 0.6662\n",
      "Epoch [55/100], Loss: 0.5901\n",
      "Epoch [56/100], Loss: 0.2639\n",
      "Epoch [57/100], Loss: 0.6774\n",
      "Epoch [58/100], Loss: 0.4278\n",
      "Epoch [59/100], Loss: 1.1513\n",
      "Epoch [60/100], Loss: 0.1780\n",
      "Epoch [61/100], Loss: 0.4565\n",
      "Epoch [62/100], Loss: 0.4468\n",
      "Epoch [63/100], Loss: 0.4331\n",
      "Epoch [64/100], Loss: 0.3478\n",
      "Epoch [65/100], Loss: 0.8857\n",
      "Epoch [66/100], Loss: 0.8017\n",
      "Epoch [67/100], Loss: 0.4807\n",
      "Epoch [68/100], Loss: 0.6927\n",
      "Epoch [69/100], Loss: 0.3815\n",
      "Epoch [70/100], Loss: 0.6155\n",
      "Epoch [71/100], Loss: 0.5879\n",
      "Epoch [72/100], Loss: 0.5660\n",
      "Epoch [73/100], Loss: 0.3195\n",
      "Epoch [74/100], Loss: 0.6097\n",
      "Epoch [75/100], Loss: 0.4506\n",
      "Epoch [76/100], Loss: 0.5112\n",
      "Epoch [77/100], Loss: 0.5225\n",
      "Epoch [78/100], Loss: 0.5615\n",
      "Epoch [79/100], Loss: 0.3736\n",
      "Epoch [80/100], Loss: 0.4634\n",
      "Epoch [81/100], Loss: 1.2761\n",
      "Epoch [82/100], Loss: 0.3874\n",
      "Epoch [83/100], Loss: 0.5963\n",
      "Epoch [84/100], Loss: 0.6377\n",
      "Epoch [85/100], Loss: 0.3673\n",
      "Epoch [86/100], Loss: 0.4892\n",
      "Epoch [87/100], Loss: 0.7088\n",
      "Epoch [88/100], Loss: 0.3358\n",
      "Epoch [89/100], Loss: 0.6956\n",
      "Epoch [90/100], Loss: 0.2331\n",
      "Epoch [91/100], Loss: 0.4671\n",
      "Epoch [92/100], Loss: 0.6111\n",
      "Epoch [93/100], Loss: 0.3782\n",
      "Epoch [94/100], Loss: 0.3073\n",
      "Epoch [95/100], Loss: 0.3875\n",
      "Epoch [96/100], Loss: 0.3981\n",
      "Epoch [97/100], Loss: 0.7771\n",
      "Epoch [98/100], Loss: 0.2215\n",
      "Epoch [99/100], Loss: 0.4359\n",
      "Epoch [100/100], Loss: 0.5458\n",
      "Mean Squared Error: 0.5546\n",
      "le temps d'exécution est de : 57.52178621292114 secondes\n"
     ]
    }
   ],
   "source": [
    "import time as tm\n",
    "\n",
    "start=tm.time()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Charger les données California Housing\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Effectuer une normalisation des données\n",
    "scaler = StandardScaler()\n",
    "data.data = scaler.fit_transform(data.data)\n",
    "\n",
    "# Convertir les données en tenseurs PyTorch\n",
    "inputs = torch.tensor(data.data, dtype=torch.float32)\n",
    "targets = torch.tensor(data.target, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Définir une classe personnalisée pour le jeu de données\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# Créer une instance du jeu de données\n",
    "dataset = CaliforniaHousingDataset(inputs, targets)\n",
    "\n",
    "# Définir la taille du lot (batch size) et créer le chargeur de données (data loader)\n",
    "dataloader = DataLoader(dataset, batch_size=42, shuffle=True)\n",
    "\n",
    "# Définir le modèle de régression linéaire\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Créer une instance du modèle\n",
    "input_size = inputs.shape[1]\n",
    "output_size = 1\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# Définir la fonction de perte (loss function) et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraînement du modèle\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # Remise à zéro des gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prédiction du modèle\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        # Calcul de la perte\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Rétropropagation et mise à jour des poids\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Affichage de la perte à chaque époque\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_targets = model(inputs)\n",
    "    mse = criterion(predicted_targets, targets)\n",
    "    print(f'Mean Squared Error: {mse.item():.4f}')\n",
    "\n",
    "end=tm.time()\n",
    "t=end - start\n",
    "\n",
    "print(\"le temps d'exécution est de :\",t,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26bcf297",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression with Boosted Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b79507e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca0350c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: iter\n",
      "[LightGBM] [Warning] Unknown parameter: iter\n",
      "[LightGBM] [Info] Total Bins 1838\n",
      "[LightGBM] [Info] Number of data points in the train set: 16512, number of used features: 8\n",
      "[LightGBM] [Info] Start training from score 2.071947\n",
      "Mean Squared Error: 0.5167637932907595\n",
      "Execution Time: 0.33496928215026855 seconds\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Register the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a LightGBM dataset for the training set\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Specify the hyperparameters for the model\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.01,\n",
    "    'iter': 100,\n",
    "     'force_col_wise': True\n",
    "}\n",
    "\n",
    "# Train the LightGBM regression model\n",
    "model = lgb.train(params, train_data)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Register the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbf0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost  Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "397bb652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2649677467129458\n",
      "Execution Time: 0.7959764003753662 seconds\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Register the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6)\n",
    "                          \n",
    "# Train the regression model\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Register the end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total execution time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time\n",
    "print(\"Execution Time:\", execution_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed92ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b845067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.22458289556216388\n",
      "le temps d'excution est de  1.8089203834533691\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Charger la dataset California Housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Créer un modèle de régression XGBoost\n",
    "model = xgb.XGBRegressor(n_estimators=100)\n",
    "\n",
    "\n",
    "# Entraîner le modèle de régression XGBoost\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# es prédictions sur la partie  de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "end_time=time.time()\n",
    "\n",
    "t=end_time-start_time\n",
    "\n",
    "print(\"le temps d'excution est de \",t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061af198",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ANN_Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d23482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sickit-learn ANN_REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "712cbcf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5639347829918976\n",
      "Temps d'exécution: 10.586100816726685 secondes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import time \n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.neural_network import MLPRegressor #Multi-Layer Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Train the model\n",
    "model = MLPRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Calculate the mean squared error of the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a16104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras ANN Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74a66c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16512 samples\n",
      "Epoch 1/100\n",
      "16512/16512 [==============================] - 1s 44us/sample - loss: 1.1882\n",
      "Epoch 2/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.4552\n",
      "Epoch 3/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3920\n",
      "Epoch 4/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3699\n",
      "Epoch 5/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3618\n",
      "Epoch 6/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3493\n",
      "Epoch 7/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3326\n",
      "Epoch 8/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3269\n",
      "Epoch 9/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3144\n",
      "Epoch 10/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3219\n",
      "Epoch 11/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.3046\n",
      "Epoch 12/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3032\n",
      "Epoch 13/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3014\n",
      "Epoch 14/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.3114\n",
      "Epoch 15/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3031\n",
      "Epoch 16/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2889\n",
      "Epoch 17/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2862\n",
      "Epoch 18/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2821\n",
      "Epoch 19/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2898\n",
      "Epoch 20/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2797\n",
      "Epoch 21/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2803\n",
      "Epoch 22/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2812\n",
      "Epoch 23/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2745\n",
      "Epoch 24/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2788\n",
      "Epoch 25/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2744\n",
      "Epoch 26/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2716\n",
      "Epoch 27/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2694\n",
      "Epoch 28/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2689\n",
      "Epoch 29/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2843\n",
      "Epoch 30/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2700\n",
      "Epoch 31/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2644\n",
      "Epoch 32/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2654\n",
      "Epoch 33/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2617\n",
      "Epoch 34/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2614\n",
      "Epoch 35/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2609\n",
      "Epoch 36/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2605\n",
      "Epoch 37/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2585\n",
      "Epoch 38/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2649\n",
      "Epoch 39/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2611\n",
      "Epoch 40/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2565\n",
      "Epoch 41/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2524\n",
      "Epoch 42/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2561\n",
      "Epoch 43/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2551\n",
      "Epoch 44/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2527\n",
      "Epoch 45/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2537\n",
      "Epoch 46/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2512\n",
      "Epoch 47/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2494\n",
      "Epoch 48/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2466\n",
      "Epoch 49/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2481\n",
      "Epoch 50/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2470\n",
      "Epoch 51/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2452\n",
      "Epoch 52/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2460\n",
      "Epoch 53/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2481\n",
      "Epoch 54/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2461\n",
      "Epoch 55/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2460\n",
      "Epoch 56/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2488\n",
      "Epoch 57/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2449\n",
      "Epoch 58/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2439\n",
      "Epoch 59/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2415\n",
      "Epoch 60/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2413\n",
      "Epoch 61/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2393\n",
      "Epoch 62/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2380\n",
      "Epoch 63/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2381\n",
      "Epoch 64/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2370\n",
      "Epoch 65/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2364\n",
      "Epoch 66/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2359\n",
      "Epoch 67/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2354\n",
      "Epoch 68/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2364\n",
      "Epoch 69/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2447\n",
      "Epoch 70/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2340\n",
      "Epoch 71/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2346\n",
      "Epoch 72/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2339\n",
      "Epoch 73/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2313\n",
      "Epoch 74/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2337\n",
      "Epoch 75/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2295\n",
      "Epoch 76/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2301\n",
      "Epoch 77/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2306\n",
      "Epoch 78/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2295\n",
      "Epoch 79/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2285\n",
      "Epoch 80/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2270\n",
      "Epoch 81/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2266\n",
      "Epoch 82/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2272\n",
      "Epoch 83/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2277\n",
      "Epoch 84/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2263\n",
      "Epoch 85/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2246\n",
      "Epoch 86/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2266\n",
      "Epoch 87/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.2234\n",
      "Epoch 88/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2295\n",
      "Epoch 89/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2253\n",
      "Epoch 90/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2229\n",
      "Epoch 91/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2247\n",
      "Epoch 92/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2231\n",
      "Epoch 93/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2234\n",
      "Epoch 94/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2220\n",
      "Epoch 95/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2210\n",
      "Epoch 96/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2228\n",
      "Epoch 97/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.2205\n",
      "Epoch 98/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2215\n",
      "Epoch 99/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2195\n",
      "Epoch 100/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.2188\n",
      "Mean Squared Error (MSE): 0.2701960192747818\n",
      "Temps d'exécution: 43.40803098678589 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time \n",
    "\n",
    "start_time=time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Chargement des données California Housing\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Normalisation des caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fractionnement des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error (MSE): {loss}\")\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch ANN Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow ANN with Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e9bcf5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16512 samples\n",
      "Epoch 1/100\n",
      "16512/16512 [==============================] - 1s 56us/sample - loss: 1.4052\n",
      "Epoch 2/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.5269\n",
      "Epoch 3/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.4371\n",
      "Epoch 4/100\n",
      "16512/16512 [==============================] - 1s 37us/sample - loss: 0.4103\n",
      "Epoch 5/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.4001\n",
      "Epoch 6/100\n",
      "16512/16512 [==============================] - 1s 41us/sample - loss: 0.3888\n",
      "Epoch 7/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.3831\n",
      "Epoch 8/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3745\n",
      "Epoch 9/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3722\n",
      "Epoch 10/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3669\n",
      "Epoch 11/100\n",
      "16512/16512 [==============================] - 1s 42us/sample - loss: 0.3602\n",
      "Epoch 12/100\n",
      "16512/16512 [==============================] - 1s 48us/sample - loss: 0.3573\n",
      "Epoch 13/100\n",
      "16512/16512 [==============================] - 1s 42us/sample - loss: 0.3522\n",
      "Epoch 14/100\n",
      "16512/16512 [==============================] - 1s 48us/sample - loss: 0.3469\n",
      "Epoch 15/100\n",
      "16512/16512 [==============================] - 1s 48us/sample - loss: 0.3424\n",
      "Epoch 16/100\n",
      "16512/16512 [==============================] - 1s 42us/sample - loss: 0.3405\n",
      "Epoch 17/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3358\n",
      "Epoch 18/100\n",
      "16512/16512 [==============================] - 1s 40us/sample - loss: 0.3368\n",
      "Epoch 19/100\n",
      "16512/16512 [==============================] - 1s 38us/sample - loss: 0.3321\n",
      "Epoch 20/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3310\n",
      "Epoch 21/100\n",
      "16512/16512 [==============================] - 1s 42us/sample - loss: 0.3282\n",
      "Epoch 22/100\n",
      "16512/16512 [==============================] - 1s 40us/sample - loss: 0.3263\n",
      "Epoch 23/100\n",
      "16512/16512 [==============================] - 1s 38us/sample - loss: 0.3259\n",
      "Epoch 24/100\n",
      "16512/16512 [==============================] - 1s 41us/sample - loss: 0.3241\n",
      "Epoch 25/100\n",
      "16512/16512 [==============================] - 1s 41us/sample - loss: 0.3336\n",
      "Epoch 26/100\n",
      "16512/16512 [==============================] - 1s 38us/sample - loss: 0.3214\n",
      "Epoch 27/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.3208\n",
      "Epoch 28/100\n",
      "16512/16512 [==============================] - 1s 35us/sample - loss: 0.3188\n",
      "Epoch 29/100\n",
      "16512/16512 [==============================] - 1s 37us/sample - loss: 0.3169\n",
      "Epoch 30/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.3187\n",
      "Epoch 31/100\n",
      "16512/16512 [==============================] - 1s 37us/sample - loss: 0.3173\n",
      "Epoch 32/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.3154\n",
      "Epoch 33/100\n",
      "16512/16512 [==============================] - 1s 41us/sample - loss: 0.3143\n",
      "Epoch 34/100\n",
      "16512/16512 [==============================] - 1s 40us/sample - loss: 0.3194\n",
      "Epoch 35/100\n",
      "16512/16512 [==============================] - 1s 40us/sample - loss: 0.3141\n",
      "Epoch 36/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.3134\n",
      "Epoch 37/100\n",
      "16512/16512 [==============================] - 1s 45us/sample - loss: 0.3135\n",
      "Epoch 38/100\n",
      "16512/16512 [==============================] - 1s 41us/sample - loss: 0.3099\n",
      "Epoch 39/100\n",
      "16512/16512 [==============================] - 0s 30us/sample - loss: 0.3119\n",
      "Epoch 40/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3103\n",
      "Epoch 41/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.3087\n",
      "Epoch 42/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3092\n",
      "Epoch 43/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.3080\n",
      "Epoch 44/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3087\n",
      "Epoch 45/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3130\n",
      "Epoch 46/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3134\n",
      "Epoch 47/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.3086\n",
      "Epoch 48/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.3065\n",
      "Epoch 49/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.3075\n",
      "Epoch 50/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.3074\n",
      "Epoch 51/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3068\n",
      "Epoch 52/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.3041\n",
      "Epoch 53/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.3052\n",
      "Epoch 54/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.3032\n",
      "Epoch 55/100\n",
      "16512/16512 [==============================] - 0s 17us/sample - loss: 0.3064\n",
      "Epoch 56/100\n",
      "16512/16512 [==============================] - 0s 17us/sample - loss: 0.3032\n",
      "Epoch 57/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.3049\n",
      "Epoch 58/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.3032\n",
      "Epoch 59/100\n",
      "16512/16512 [==============================] - 0s 13us/sample - loss: 0.3029\n",
      "Epoch 60/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.3027\n",
      "Epoch 61/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3023\n",
      "Epoch 62/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3026\n",
      "Epoch 63/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3007\n",
      "Epoch 64/100\n",
      "16512/16512 [==============================] - 0s 15us/sample - loss: 0.3002\n",
      "Epoch 65/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.3080\n",
      "Epoch 66/100\n",
      "16512/16512 [==============================] - 0s 14us/sample - loss: 0.2994\n",
      "Epoch 67/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2995\n",
      "Epoch 68/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.3002\n",
      "Epoch 69/100\n",
      "16512/16512 [==============================] - 0s 17us/sample - loss: 0.3005\n",
      "Epoch 70/100\n",
      "16512/16512 [==============================] - 0s 13us/sample - loss: 0.3018\n",
      "Epoch 71/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.2975\n",
      "Epoch 72/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.3015\n",
      "Epoch 73/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2994\n",
      "Epoch 74/100\n",
      "16512/16512 [==============================] - 1s 34us/sample - loss: 0.2980\n",
      "Epoch 75/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2972\n",
      "Epoch 76/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.2977\n",
      "Epoch 77/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.2978\n",
      "Epoch 78/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3021\n",
      "Epoch 79/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2973\n",
      "Epoch 80/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.2972\n",
      "Epoch 81/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.2971\n",
      "Epoch 82/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.2994\n",
      "Epoch 83/100\n",
      "16512/16512 [==============================] - 1s 33us/sample - loss: 0.2960\n",
      "Epoch 84/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.2965\n",
      "Epoch 85/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2962\n",
      "Epoch 86/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.2959\n",
      "Epoch 87/100\n",
      "16512/16512 [==============================] - 0s 27us/sample - loss: 0.2944\n",
      "Epoch 88/100\n",
      "16512/16512 [==============================] - 1s 39us/sample - loss: 0.2968\n",
      "Epoch 89/100\n",
      "16512/16512 [==============================] - 0s 28us/sample - loss: 0.2963\n",
      "Epoch 90/100\n",
      "16512/16512 [==============================] - 0s 29us/sample - loss: 0.3001\n",
      "Epoch 91/100\n",
      "16512/16512 [==============================] - 0s 30us/sample - loss: 0.2944\n",
      "Epoch 92/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.2961\n",
      "Epoch 93/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 0.3056\n",
      "Epoch 94/100\n",
      "16512/16512 [==============================] - 1s 36us/sample - loss: 0.2945\n",
      "Epoch 95/100\n",
      "16512/16512 [==============================] - 1s 33us/sample - loss: 0.2921\n",
      "Epoch 96/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2939\n",
      "Epoch 97/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.2926\n",
      "Epoch 98/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.2933\n",
      "Epoch 99/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.2936\n",
      "Epoch 100/100\n",
      "16512/16512 [==============================] - 0s 14us/sample - loss: 0.2931\n",
      "Mean Squared Error: 0.31402629353972367\n",
      "Le temps d'exécution est de  49.19901156425476 secondes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    }
   ],
   "source": [
    "import time as tm\n",
    "\n",
    "start=tm.time()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "end=tm.time()\n",
    "\n",
    "t=end - start \n",
    "\n",
    "print(\"Le temps d'exécution est de \",t,\"secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7ac3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow ANN Regression low level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e60e250",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilham\\AppData\\Local\\Temp\\ipykernel_1188\\138326924.py:37: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  hidden = tf.layers.dense(X, n_neurons, activation=tf.nn.relu)\n",
      "C:\\Users\\ilham\\AppData\\Local\\Temp\\ipykernel_1188\\138326924.py:40: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  output = tf.layers.dense(hidden, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.4137963056564331\n",
      "Epoch 1: Train Loss = 0.39138063788414\n",
      "Epoch 2: Train Loss = 0.3723858892917633\n",
      "Epoch 3: Train Loss = 0.351890504360199\n",
      "Epoch 4: Train Loss = 0.34325143694877625\n",
      "Epoch 5: Train Loss = 0.3384319245815277\n",
      "Epoch 6: Train Loss = 0.3352556526660919\n",
      "Epoch 7: Train Loss = 0.333845853805542\n",
      "Epoch 8: Train Loss = 0.33150967955589294\n",
      "Epoch 9: Train Loss = 0.331399142742157\n",
      "Epoch 10: Train Loss = 0.32933753728866577\n",
      "Epoch 11: Train Loss = 0.3277006447315216\n",
      "Epoch 12: Train Loss = 0.321413516998291\n",
      "Epoch 13: Train Loss = 0.3206033706665039\n",
      "Epoch 14: Train Loss = 0.31688666343688965\n",
      "Epoch 15: Train Loss = 0.31334421038627625\n",
      "Epoch 16: Train Loss = 0.3144668638706207\n",
      "Epoch 17: Train Loss = 0.316101610660553\n",
      "Epoch 18: Train Loss = 0.3063182830810547\n",
      "Epoch 19: Train Loss = 0.3043670356273651\n",
      "Epoch 20: Train Loss = 0.300555020570755\n",
      "Epoch 21: Train Loss = 0.30181145668029785\n",
      "Epoch 22: Train Loss = 0.3009580373764038\n",
      "Epoch 23: Train Loss = 0.29734063148498535\n",
      "Epoch 24: Train Loss = 0.2995193898677826\n",
      "Epoch 25: Train Loss = 0.29725420475006104\n",
      "Epoch 26: Train Loss = 0.29528751969337463\n",
      "Epoch 27: Train Loss = 0.29625245928764343\n",
      "Epoch 28: Train Loss = 0.29445889592170715\n",
      "Epoch 29: Train Loss = 0.29559680819511414\n",
      "Epoch 30: Train Loss = 0.31387826800346375\n",
      "Epoch 31: Train Loss = 0.29617348313331604\n",
      "Epoch 32: Train Loss = 0.29439467191696167\n",
      "Epoch 33: Train Loss = 0.2910388112068176\n",
      "Epoch 34: Train Loss = 0.2923832833766937\n",
      "Epoch 35: Train Loss = 0.2920796871185303\n",
      "Epoch 36: Train Loss = 0.29291290044784546\n",
      "Epoch 37: Train Loss = 0.2964860796928406\n",
      "Epoch 38: Train Loss = 0.2940250635147095\n",
      "Epoch 39: Train Loss = 0.29138070344924927\n",
      "Epoch 40: Train Loss = 0.2964514493942261\n",
      "Epoch 41: Train Loss = 0.28988662362098694\n",
      "Epoch 42: Train Loss = 0.29388388991355896\n",
      "Epoch 43: Train Loss = 0.28983157873153687\n",
      "Epoch 44: Train Loss = 0.29372021555900574\n",
      "Epoch 45: Train Loss = 0.28974345326423645\n",
      "Epoch 46: Train Loss = 0.28943508863449097\n",
      "Epoch 47: Train Loss = 0.28981590270996094\n",
      "Epoch 48: Train Loss = 0.28848350048065186\n",
      "Epoch 49: Train Loss = 0.2962435483932495\n",
      "Epoch 50: Train Loss = 0.29046159982681274\n",
      "Epoch 51: Train Loss = 0.2924050986766815\n",
      "Epoch 52: Train Loss = 0.2937123477458954\n",
      "Epoch 53: Train Loss = 0.2907586991786957\n",
      "Epoch 54: Train Loss = 0.29016050696372986\n",
      "Epoch 55: Train Loss = 0.2943268120288849\n",
      "Epoch 56: Train Loss = 0.2919789254665375\n",
      "Epoch 57: Train Loss = 0.2911142110824585\n",
      "Epoch 58: Train Loss = 0.28949350118637085\n",
      "Epoch 59: Train Loss = 0.2869507670402527\n",
      "Epoch 60: Train Loss = 0.2964267134666443\n",
      "Epoch 61: Train Loss = 0.2884541153907776\n",
      "Epoch 62: Train Loss = 0.2889900505542755\n",
      "Epoch 63: Train Loss = 0.2864518463611603\n",
      "Epoch 64: Train Loss = 0.2870730757713318\n",
      "Epoch 65: Train Loss = 0.2867632210254669\n",
      "Epoch 66: Train Loss = 0.288106232881546\n",
      "Epoch 67: Train Loss = 0.28798580169677734\n",
      "Epoch 68: Train Loss = 0.2853368818759918\n",
      "Epoch 69: Train Loss = 0.2866954505443573\n",
      "Epoch 70: Train Loss = 0.28634771704673767\n",
      "Epoch 71: Train Loss = 0.2846447825431824\n",
      "Epoch 72: Train Loss = 0.2912079095840454\n",
      "Epoch 73: Train Loss = 0.2879476547241211\n",
      "Epoch 74: Train Loss = 0.28808850049972534\n",
      "Epoch 75: Train Loss = 0.2880703806877136\n",
      "Epoch 76: Train Loss = 0.29043442010879517\n",
      "Epoch 77: Train Loss = 0.28862136602401733\n",
      "Epoch 78: Train Loss = 0.2859884798526764\n",
      "Epoch 79: Train Loss = 0.28766337037086487\n",
      "Epoch 80: Train Loss = 0.2851775288581848\n",
      "Epoch 81: Train Loss = 0.2833808362483978\n",
      "Epoch 82: Train Loss = 0.28467926383018494\n",
      "Epoch 83: Train Loss = 0.2853459119796753\n",
      "Epoch 84: Train Loss = 0.28453755378723145\n",
      "Epoch 85: Train Loss = 0.2851768136024475\n",
      "Epoch 86: Train Loss = 0.284621000289917\n",
      "Epoch 87: Train Loss = 0.28278225660324097\n",
      "Epoch 88: Train Loss = 0.28280478715896606\n",
      "Epoch 89: Train Loss = 0.2845029830932617\n",
      "Epoch 90: Train Loss = 0.28975534439086914\n",
      "Epoch 91: Train Loss = 0.34011468291282654\n",
      "Epoch 92: Train Loss = 0.2894004285335541\n",
      "Epoch 93: Train Loss = 0.28797516226768494\n",
      "Epoch 94: Train Loss = 0.285418838262558\n",
      "Epoch 95: Train Loss = 0.2833932638168335\n",
      "Epoch 96: Train Loss = 0.28494346141815186\n",
      "Epoch 97: Train Loss = 0.28484848141670227\n",
      "Epoch 98: Train Loss = 0.2833631932735443\n",
      "Epoch 99: Train Loss = 0.3089541494846344\n",
      "Test Loss = 0.3110746741294861\n",
      "le temps d'exécution est de  19.932138204574585 secondes\n"
     ]
    }
   ],
   "source": [
    "import time as tm\n",
    "\n",
    "start = tm.time()\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.disable_v2_behavior()  # Enable compatibility with TensorFlow 1.x\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "n_features = X_train_scaled.shape[1]\n",
    "n_neurons = 30\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Define placeholders for inputs and targets\n",
    "X = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, n_features), name='X')\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "# Define the hidden layer\n",
    "hidden = tf.layers.dense(X, n_neurons, activation=tf.nn.relu)\n",
    "\n",
    "# Define the output layer\n",
    "output = tf.layers.dense(hidden, 1)\n",
    "\n",
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.square(output - y))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Create a session and initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(X_train_scaled.shape[0] // batch_size):\n",
    "        X_batch = X_train_scaled[iteration*batch_size:(iteration+1)*batch_size]\n",
    "        y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    if epoch % 1== 0:\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train_scaled, y: y_train})\n",
    "        print(f'Epoch {epoch}: Train Loss = {train_loss}')\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = sess.run(loss, feed_dict={X: X_test_scaled, y: y_test})\n",
    "print(f'Test Loss = {test_loss}')\n",
    "\n",
    "end = tm.time()\n",
    "\n",
    "t=end-start\n",
    "print(\"le temps d'exécution est de \",t,\"secondes\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d032678",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f7264",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hp\\OneDrive\\Bureau\\PFA\\PFA_VF\\PFA_Code_Source.ipynb Cell 61\u001b[0m in \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Bureau/PFA/PFA_VF/PFA_Code_Source.ipynb#Y114sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Bureau/PFA/PFA_VF/PFA_Code_Source.ipynb#Y114sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Bureau/PFA/PFA_VF/PFA_Code_Source.ipynb#Y114sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_iris\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Bureau/PFA/PFA_VF/PFA_Code_Source.ipynb#Y114sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hp/OneDrive/Bureau/PFA/PFA_VF/PFA_Code_Source.ipynb#Y114sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeClassifier\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Enregistrer le temps de départ\n",
    "start_time1 = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score  \n",
    "\n",
    "# Code dont vous voulez mesurer le temps d'exécution\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "tree_clf = DecisionTreeClassifier (max_depth=6)\n",
    "\n",
    "# Train the model\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree_clf.predict(X_test)\n",
    "\n",
    "# Measure the accuracy of the model\n",
    "accuracy1 = accuracy_score(y_test, y_pred)\n",
    "print(\"scikit-learn Accuracy:\", accuracy1)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time1 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time1 = end_time1 - start_time1\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time1, \"secondes\")\n",
    "\n",
    "import time\n",
    "# Enregistrer le temps de départ\n",
    "start_time2 = time.time()\n",
    "\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of XGBClassifier\n",
    "model = xgb.XGBClassifier(objective='multiclass', num_class=3,n_estimators = 100, max_depth=6, learning_rate=0.1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy2 = accuracy_score(y_test, y_pred)\n",
    "print(\"XGBoost Accuracy:\", accuracy2)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time2 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time2 = end_time2 - start_time2\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time2, \"secondes\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time3 = time.time()\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# charger dataset\n",
    "iris = load_iris()\n",
    "\n",
    "\n",
    "# diviser les données en 2 parties , une pour apprentissage et l’autre cpour le test \n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Entrainement du modèle\n",
    "lgbm = LGBMClassifier(objective='multiclass', num_class=3, max_depth=6, learning_rate=0.1)\n",
    "lgbm.fit(X_train ,y_train)\n",
    "\n",
    "\n",
    "# Model Accuracy  \n",
    "lgbm_pred = lgbm.predict(X_test)\n",
    "accuracy3 = accuracy_score(y_test, lgbm_pred)\n",
    "print(\"LightGBM Accuracy:\", accuracy3)\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time3 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time3 = end_time3 - start_time3\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time3,\"secondes\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time4 = time.time()\n",
    "\n",
    "import catboost as cb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Charger les données de l'ensemble de données Iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Créer un modèle de classification avec Catboost\n",
    "model = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, loss_function='MultiClass')\n",
    "\n",
    "\n",
    "# Entraînement du modèle \n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "\n",
    "# Prédire les  classes pour les données de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Évaluer la précision du modèle \n",
    "accuracy4 = accuracy_score(y_test, y_pred)\n",
    "print(\" CatBoost Accuracy:\", accuracy4)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time4 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time4 = end_time4 - start_time4\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time4,\"secondes\")\n",
    "\n",
    "# Create lists to store accuracy and execution time\n",
    "accuracy_list = [accuracy1, accuracy2, accuracy3, accuracy4]\n",
    "execution_time_list = [execution_time1, execution_time2, execution_time3, execution_time4]\n",
    "\n",
    "# Plotting the bar graph\n",
    "labels = ['scikit-learn', 'XGBoost', 'LightGBM', 'CatBoost']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(x - width/2, accuracy_list, width, label='Accuracy', color= '#8AC847')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, execution_time_list, width, label='Execution Time', color='pink')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "\n",
    "fig.suptitle('Accuracy and Execution Time Comparison for decision trees algorithm')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f84833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANN CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d57cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333\n",
      "Temps d'exécution: 0.03589606285095215 secondes\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 2ms/step - loss: 0.9603 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.7044 - accuracy: 0.7083\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6025 - accuracy: 0.6667\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.5260 - accuracy: 0.8083\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.4704 - accuracy: 0.7917\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.4332 - accuracy: 0.8083\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8667\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.3600 - accuracy: 0.8833\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.9250\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.9250\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9667\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.9250\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2309 - accuracy: 0.9500\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9583\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9250\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9500\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1893 - accuracy: 0.9417\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9750\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9417\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9417\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9500\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9667\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9750\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9750\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.1378 - accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1238 - accuracy: 0.9500\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9500\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1274 - accuracy: 0.9500\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9667\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9750\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9583\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9583\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9417\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9500\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9750\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9667\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9583\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9417\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9500\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0878 - accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9583\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9667\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9583\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0933 - accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9500\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0961 - accuracy: 0.9583\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0903 - accuracy: 0.9500\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9833\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9583\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0839 - accuracy: 0.9750\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9667\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9583\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0812 - accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9583\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0860 - accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9500\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9583\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0786 - accuracy: 0.9833\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9750\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0989 - accuracy: 0.9583\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0688 - accuracy: 0.9667\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1091 - accuracy: 0.9417\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0738 - accuracy: 0.9750\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9667\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9750\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0911 - accuracy: 0.9583\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0558 - accuracy: 0.9833\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1196 - accuracy: 0.9417\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0916 - accuracy: 0.9500\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0871 - accuracy: 0.9500\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9667\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9583\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9833\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0961 - accuracy: 0.9500\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9583\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9667\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9583\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9417\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9667\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9583\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0790 - accuracy: 0.9583\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9667\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0914 - accuracy: 0.9500\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9583\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9583\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0837 - accuracy: 0.9750\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9583\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0815 - accuracy: 0.9583\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9583\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9833\n",
      "1/1 [==============================] - 0s 153ms/step - loss: 0.0731 - accuracy: 0.9333\n",
      "Test Accuracy:  0.9333333373069763\n",
      "Temps d'exécution: 3.42521333694458 secondes\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 16ms/step - loss: 1.0184 - accuracy: 0.4417 - val_loss: 0.8799 - val_accuracy: 0.7000\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8185 - accuracy: 0.6583 - val_loss: 0.7266 - val_accuracy: 0.7000\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.7333 - val_loss: 0.6158 - val_accuracy: 0.7667\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.5891 - accuracy: 0.7417 - val_loss: 0.5241 - val_accuracy: 0.8000\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.5188 - accuracy: 0.7000 - val_loss: 0.4708 - val_accuracy: 0.8333\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4655 - accuracy: 0.9333 - val_loss: 0.4480 - val_accuracy: 0.8667\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4210 - accuracy: 0.8667 - val_loss: 0.3982 - val_accuracy: 0.8000\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3989 - accuracy: 0.8917 - val_loss: 0.3714 - val_accuracy: 0.9667\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3608 - accuracy: 0.9250 - val_loss: 0.3440 - val_accuracy: 0.9667\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.3325 - accuracy: 0.9667 - val_loss: 0.3210 - val_accuracy: 0.9667\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3058 - accuracy: 0.9500 - val_loss: 0.2972 - val_accuracy: 0.9667\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2849 - accuracy: 0.9500 - val_loss: 0.2776 - val_accuracy: 0.9667\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2632 - accuracy: 0.9667 - val_loss: 0.2576 - val_accuracy: 0.9667\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2441 - accuracy: 0.9583 - val_loss: 0.2657 - val_accuracy: 0.8667\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2326 - accuracy: 0.9500 - val_loss: 0.2263 - val_accuracy: 0.9667\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2144 - accuracy: 0.9667 - val_loss: 0.2142 - val_accuracy: 0.9667\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1940 - accuracy: 0.9667 - val_loss: 0.2104 - val_accuracy: 0.9667\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1828 - accuracy: 0.9833 - val_loss: 0.2001 - val_accuracy: 0.9667\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1692 - accuracy: 0.9667 - val_loss: 0.1844 - val_accuracy: 0.9667\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1663 - accuracy: 0.9750 - val_loss: 0.1747 - val_accuracy: 0.9667\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1608 - accuracy: 0.9667 - val_loss: 0.1685 - val_accuracy: 0.9667\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1436 - accuracy: 0.9750 - val_loss: 0.1819 - val_accuracy: 0.8667\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1431 - accuracy: 0.9667 - val_loss: 0.1554 - val_accuracy: 0.9667\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1274 - accuracy: 0.9833 - val_loss: 0.1713 - val_accuracy: 0.9000\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1269 - accuracy: 0.9750 - val_loss: 0.1413 - val_accuracy: 0.9667\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9667 - val_loss: 0.1368 - val_accuracy: 0.9667\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1234 - accuracy: 0.9750 - val_loss: 0.1785 - val_accuracy: 0.8667\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1110 - accuracy: 0.9750 - val_loss: 0.1280 - val_accuracy: 0.9667\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1126 - accuracy: 0.9750 - val_loss: 0.1256 - val_accuracy: 0.9667\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1227 - accuracy: 0.9583 - val_loss: 0.1219 - val_accuracy: 0.9667\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1111 - accuracy: 0.9667 - val_loss: 0.1632 - val_accuracy: 0.8667\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1006 - accuracy: 0.9667 - val_loss: 0.1152 - val_accuracy: 0.9667\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0998 - accuracy: 0.9750 - val_loss: 0.1206 - val_accuracy: 0.9667\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0973 - accuracy: 0.9750 - val_loss: 0.1154 - val_accuracy: 0.9667\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1071 - accuracy: 0.9583 - val_loss: 0.1169 - val_accuracy: 0.9667\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1192 - accuracy: 0.9500 - val_loss: 0.1593 - val_accuracy: 0.8667\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1174 - accuracy: 0.9500 - val_loss: 0.1047 - val_accuracy: 0.9667\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9667 - val_loss: 0.1109 - val_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9667 - val_loss: 0.1038 - val_accuracy: 0.9667\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0887 - accuracy: 0.9750 - val_loss: 0.1132 - val_accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0847 - accuracy: 0.9667 - val_loss: 0.1008 - val_accuracy: 0.9667\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0917 - accuracy: 0.9667 - val_loss: 0.0963 - val_accuracy: 0.9667\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1003 - accuracy: 0.9667 - val_loss: 0.0948 - val_accuracy: 0.9667\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0958 - accuracy: 0.9583 - val_loss: 0.1296 - val_accuracy: 0.9000\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9750 - val_loss: 0.0942 - val_accuracy: 0.9667\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0952 - accuracy: 0.9583 - val_loss: 0.0935 - val_accuracy: 0.9667\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9583 - val_loss: 0.1733 - val_accuracy: 0.8667\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0767 - accuracy: 0.9833 - val_loss: 0.0895 - val_accuracy: 0.9667\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0827 - accuracy: 0.9750 - val_loss: 0.0903 - val_accuracy: 0.9667\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0776 - accuracy: 0.9750 - val_loss: 0.0958 - val_accuracy: 0.9667\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0756 - accuracy: 0.9750 - val_loss: 0.0872 - val_accuracy: 0.9667\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0816 - accuracy: 0.9833 - val_loss: 0.1145 - val_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.9583 - val_loss: 0.0853 - val_accuracy: 0.9667\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0801 - accuracy: 0.9833 - val_loss: 0.1627 - val_accuracy: 0.9000\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0771 - accuracy: 0.9750 - val_loss: 0.0871 - val_accuracy: 0.9667\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9833 - val_loss: 0.1034 - val_accuracy: 0.9667\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0739 - accuracy: 0.9750 - val_loss: 0.1008 - val_accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0770 - accuracy: 0.9833 - val_loss: 0.0817 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0837 - accuracy: 0.9667 - val_loss: 0.0811 - val_accuracy: 0.9667\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0909 - accuracy: 0.9583 - val_loss: 0.0798 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0896 - accuracy: 0.9667 - val_loss: 0.1907 - val_accuracy: 0.8667\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0843 - accuracy: 0.9833 - val_loss: 0.0802 - val_accuracy: 0.9667\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9750 - val_loss: 0.1097 - val_accuracy: 0.9000\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0959 - accuracy: 0.9667 - val_loss: 0.0991 - val_accuracy: 0.9667\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0750 - accuracy: 0.9583 - val_loss: 0.0994 - val_accuracy: 0.9667\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0936 - accuracy: 0.9750 - val_loss: 0.0782 - val_accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0961 - accuracy: 0.9500 - val_loss: 0.0879 - val_accuracy: 0.9667\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1126 - accuracy: 0.9583 - val_loss: 0.1712 - val_accuracy: 0.9000\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0894 - accuracy: 0.9667 - val_loss: 0.0759 - val_accuracy: 0.9667\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0840 - accuracy: 0.9500 - val_loss: 0.0750 - val_accuracy: 0.9667\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9833 - val_loss: 0.0945 - val_accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9667 - val_loss: 0.0908 - val_accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0699 - accuracy: 0.9833 - val_loss: 0.0834 - val_accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0702 - accuracy: 0.9667 - val_loss: 0.0740 - val_accuracy: 0.9667\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9833 - val_loss: 0.0915 - val_accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9750 - val_loss: 0.0815 - val_accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0821 - accuracy: 0.9583 - val_loss: 0.1309 - val_accuracy: 0.9000\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1118 - accuracy: 0.9500 - val_loss: 0.1050 - val_accuracy: 0.9000\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0720 - accuracy: 0.9667 - val_loss: 0.0746 - val_accuracy: 0.9667\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0707 - accuracy: 0.9667 - val_loss: 0.0785 - val_accuracy: 0.9667\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0732 - accuracy: 0.9667 - val_loss: 0.0751 - val_accuracy: 0.9667\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0757 - accuracy: 0.9750 - val_loss: 0.1264 - val_accuracy: 0.9000\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9833 - val_loss: 0.0745 - val_accuracy: 0.9667\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1018 - accuracy: 0.9667 - val_loss: 0.1302 - val_accuracy: 0.9000\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0833 - accuracy: 0.9750 - val_loss: 0.0717 - val_accuracy: 0.9667\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0708 - accuracy: 0.9750 - val_loss: 0.0989 - val_accuracy: 0.9667\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0702 - accuracy: 0.9583 - val_loss: 0.1057 - val_accuracy: 0.9000\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0793 - accuracy: 0.9750 - val_loss: 0.0687 - val_accuracy: 0.9667\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.0937 - accuracy: 0.9500 - val_loss: 0.0704 - val_accuracy: 0.9667\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0744 - accuracy: 0.9833 - val_loss: 0.0949 - val_accuracy: 0.9667\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9667 - val_loss: 0.0862 - val_accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0690 - accuracy: 0.9667 - val_loss: 0.0782 - val_accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0680 - accuracy: 0.9750 - val_loss: 0.0803 - val_accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0731 - accuracy: 0.9583 - val_loss: 0.0677 - val_accuracy: 0.9667\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0695 - accuracy: 0.9750 - val_loss: 0.0975 - val_accuracy: 0.9333\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0728 - accuracy: 0.9583 - val_loss: 0.0677 - val_accuracy: 0.9667\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0710 - accuracy: 0.9833 - val_loss: 0.1313 - val_accuracy: 0.9000\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0809 - accuracy: 0.9500 - val_loss: 0.0674 - val_accuracy: 0.9667\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0734 - accuracy: 0.9750 - val_loss: 0.0816 - val_accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0729 - accuracy: 0.9833 - val_loss: 0.1076 - val_accuracy: 0.9000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1076 - accuracy: 0.9000\n",
      "Test accuracy: 0.8999999761581421\n",
      "Temps d'exécution: 7.9195239543914795 secondes\n",
      "Accuracy: 0.9667\n",
      "Temps d'exécution: 1.431896686553955 secondes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time1 = time.time()\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Charger les données de l'Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardiser les données\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Créer le modèle de réseau de neurones\n",
    "model = MLPClassifier(hidden_layer_sizes=(10, 10), activation='relu', solver='adam', max_iter=100)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire les étiquettes pour l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculer la précision\n",
    "accuracy1 = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy1:.4f}')\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time1 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time1 = end_time1 - start_time1\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time1,\"secondes\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time2 = time.time()\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "# Create the network\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(4,)))\n",
    "network.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the network\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Load the iris dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create training and test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Create categorical labels\n",
    "\n",
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)\n",
    "\n",
    "# Fit the neural network\n",
    "\n",
    "network.fit(X_train, train_labels, epochs=100, batch_size=8)\n",
    "\n",
    "# Get the accuracy of test data set\n",
    "test_loss, accuracy2 = network.evaluate(X_test, test_labels)\n",
    "\n",
    "# Print the test accuracy\n",
    "print('Test Accuracy: ',accuracy2)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time2 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time2= end_time2 - start_time2\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time2,\"secondes\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time3 = time.time()\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a neural network model\n",
    "model = models.Sequential ([\n",
    "  layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(3, activation='softmax')])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=8, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, accuracy3 = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', accuracy3)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time3 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time3 = end_time3 - start_time3\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time3,\"secondes\")\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time5 = time.time()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "# Charger les données de l'Iris dataset\n",
    "iris_data = load_iris()\n",
    "X = iris_data.data\n",
    "y = iris_data.target\n",
    "\n",
    "# Convertir les étiquettes en nombres entiers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir les données en tenseurs PyTorch\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Créer un DataLoader pour l'ensemble d'entraînement\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Définir le modèle de réseau de neurones\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Créer une instance du modèle\n",
    "model = NeuralNet()\n",
    "\n",
    "# Définir la fonction de perte et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraîner le modèle\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Évaluer le modèle sur l'ensemble de test\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    _, predicted = torch.max(outputs, dim=1)\n",
    "    accuracy5 = (predicted == y_test).sum().item() / len(y_test)\n",
    "\n",
    "print(f'Accuracy: {accuracy5:.4f}')\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time5 = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time5= end_time5 - start_time5\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time5,\"secondes\")\n",
    "\n",
    "# Create lists to store accuracy and execution time\n",
    "accuracy_list = [accuracy1, accuracy2, accuracy3,accuracy5]\n",
    "execution_time_list = [execution_time1, execution_time2, execution_time3,execution_time5]\n",
    "\n",
    "# Plotting the bar graph\n",
    "labels = ['scikit-learn', 'keras','Tensorflow', 'pytorch']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(x - width/2, accuracy_list, width, label='Accuracy', color= '#8AC847')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, execution_time_list, width, label='Execution Time', color='pink')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "\n",
    "fig.suptitle('Accuracy and Execution Time Comparison for Neural Network algorithm (Classification)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "604fc653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803987bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.555891598695242\n",
      "Temps d'exécution: 0.01652812957763672 secondes\n",
      "Mean Squared Error: 0.6607169131080098\n",
      "Temps d'exécution: 0.08796000480651855 secondes\n",
      "Train on 16512 samples\n",
      "Epoch 1/100\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 4.9159\n",
      "Epoch 2/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 2.9671\n",
      "Epoch 3/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 1.8476\n",
      "Epoch 4/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 1.2279\n",
      "Epoch 5/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.9062\n",
      "Epoch 6/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.7554\n",
      "Epoch 7/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.6871\n",
      "Epoch 8/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.6526\n",
      "Epoch 9/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.6293\n",
      "Epoch 10/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.6092\n",
      "Epoch 11/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5908\n",
      "Epoch 12/100\n",
      "16512/16512 [==============================] - 0s 17us/sample - loss: 0.5758\n",
      "Epoch 13/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5633\n",
      "Epoch 14/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5526\n",
      "Epoch 15/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5438\n",
      "Epoch 16/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5369\n",
      "Epoch 17/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5308\n",
      "Epoch 18/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5267\n",
      "Epoch 19/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5233\n",
      "Epoch 20/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5212\n",
      "Epoch 21/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5202\n",
      "Epoch 22/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5197\n",
      "Epoch 23/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5196\n",
      "Epoch 24/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 25/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 26/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 27/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 28/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 29/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5193\n",
      "Epoch 30/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 31/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 32/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 33/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 34/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5192\n",
      "Epoch 35/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5186\n",
      "Epoch 36/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5195\n",
      "Epoch 37/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5189\n",
      "Epoch 38/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.5194\n",
      "Epoch 39/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5191\n",
      "Epoch 40/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5195\n",
      "Epoch 41/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5192\n",
      "Epoch 42/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5189\n",
      "Epoch 43/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5193\n",
      "Epoch 44/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5193\n",
      "Epoch 45/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5190\n",
      "Epoch 46/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5192\n",
      "Epoch 47/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5190\n",
      "Epoch 48/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5192\n",
      "Epoch 49/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5191\n",
      "Epoch 50/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5189\n",
      "Epoch 51/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5193\n",
      "Epoch 52/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5191\n",
      "Epoch 53/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5192\n",
      "Epoch 54/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5194\n",
      "Epoch 55/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5187\n",
      "Epoch 56/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5190\n",
      "Epoch 57/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5185\n",
      "Epoch 58/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5190\n",
      "Epoch 59/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 60/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 61/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 62/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5187\n",
      "Epoch 63/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5194\n",
      "Epoch 64/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 65/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5196\n",
      "Epoch 66/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5189\n",
      "Epoch 67/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5189\n",
      "Epoch 68/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 69/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5191\n",
      "Epoch 70/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5192\n",
      "Epoch 71/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 72/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 73/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5192\n",
      "Epoch 74/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 75/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5190\n",
      "Epoch 76/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5192\n",
      "Epoch 77/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 78/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5186\n",
      "Epoch 79/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 80/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5191\n",
      "Epoch 81/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5189\n",
      "Epoch 82/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5194\n",
      "Epoch 83/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5194\n",
      "Epoch 84/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5190\n",
      "Epoch 85/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 86/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5190\n",
      "Epoch 87/100\n",
      "16512/16512 [==============================] - 0s 13us/sample - loss: 0.5187\n",
      "Epoch 88/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 89/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5187\n",
      "Epoch 90/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5194\n",
      "Epoch 91/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 92/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 93/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 94/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5190\n",
      "Epoch 95/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 96/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5193\n",
      "Epoch 97/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5190\n",
      "Epoch 98/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 99/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5196\n",
      "Epoch 100/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Mean Squared Error (MSE): 0.5578135471473369\n",
      "le temps d'exécution est : 36.92866396903992 secondes\n",
      "Train on 16512 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16512/16512 [==============================] - 0s 27us/sample - loss: 5.2106\n",
      "Epoch 2/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 3.0010\n",
      "Epoch 3/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 1.8995\n",
      "Epoch 4/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 1.2716\n",
      "Epoch 5/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.9365\n",
      "Epoch 6/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.7757\n",
      "Epoch 7/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.7037\n",
      "Epoch 8/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.6679\n",
      "Epoch 9/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.6445\n",
      "Epoch 10/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.6241\n",
      "Epoch 11/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.6065\n",
      "Epoch 12/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5898\n",
      "Epoch 13/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5756\n",
      "Epoch 14/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5641\n",
      "Epoch 15/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5544\n",
      "Epoch 16/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5458\n",
      "Epoch 17/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5387\n",
      "Epoch 18/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5327\n",
      "Epoch 19/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5283\n",
      "Epoch 20/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5245\n",
      "Epoch 21/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5222\n",
      "Epoch 22/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5205\n",
      "Epoch 23/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5201\n",
      "Epoch 24/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5189\n",
      "Epoch 25/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 26/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 27/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5188\n",
      "Epoch 28/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 29/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 30/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5193\n",
      "Epoch 31/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 32/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5190\n",
      "Epoch 33/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 34/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 35/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 36/100\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5190\n",
      "Epoch 37/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.5189\n",
      "Epoch 38/100\n",
      "16512/16512 [==============================] - 0s 14us/sample - loss: 0.5194\n",
      "Epoch 39/100\n",
      "16512/16512 [==============================] - 0s 16us/sample - loss: 0.5189\n",
      "Epoch 40/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5189\n",
      "Epoch 41/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 42/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 43/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5193\n",
      "Epoch 44/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 45/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 46/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5189\n",
      "Epoch 47/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 48/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5194\n",
      "Epoch 49/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5187\n",
      "Epoch 50/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5194\n",
      "Epoch 51/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 52/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 53/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 54/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5189\n",
      "Epoch 55/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5196\n",
      "Epoch 56/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5191\n",
      "Epoch 57/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5190\n",
      "Epoch 58/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 59/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5195\n",
      "Epoch 60/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 61/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5189\n",
      "Epoch 62/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.5192\n",
      "Epoch 63/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.5189\n",
      "Epoch 64/100\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5193\n",
      "Epoch 65/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5194\n",
      "Epoch 66/100\n",
      "16512/16512 [==============================] - 0s 14us/sample - loss: 0.5194\n",
      "Epoch 67/100\n",
      "16512/16512 [==============================] - 0s 17us/sample - loss: 0.5192\n",
      "Epoch 68/100\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5190\n",
      "Epoch 69/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.5192\n",
      "Epoch 70/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 71/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 72/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5188\n",
      "Epoch 73/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 74/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 75/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 76/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 77/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5188\n",
      "Epoch 78/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 79/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5193\n",
      "Epoch 80/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 81/100\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.5187\n",
      "Epoch 82/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5194\n",
      "Epoch 83/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5190\n",
      "Epoch 84/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 85/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5191\n",
      "Epoch 86/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5191\n",
      "Epoch 87/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 88/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 89/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5192\n",
      "Epoch 90/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 91/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5193\n",
      "Epoch 92/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 93/100\n",
      "16512/16512 [==============================] - 0s 25us/sample - loss: 0.5195\n",
      "Epoch 94/100\n",
      "16512/16512 [==============================] - 0s 24us/sample - loss: 0.5191\n",
      "Epoch 95/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5190\n",
      "Epoch 96/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5194\n",
      "Epoch 97/100\n",
      "16512/16512 [==============================] - 0s 22us/sample - loss: 0.5192\n",
      "Epoch 98/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5192\n",
      "Epoch 99/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5195\n",
      "Epoch 100/100\n",
      "16512/16512 [==============================] - 0s 23us/sample - loss: 0.5189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "c:\\Python\\Python311\\Lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5545775891505471\n",
      "le temps d'exécution est de :   37.42886185646057 Seconds \n",
      "Epoch [1/100], Loss: 0.7805\n",
      "Epoch [2/100], Loss: 0.7437\n",
      "Epoch [3/100], Loss: 0.6652\n",
      "Epoch [4/100], Loss: 0.2654\n",
      "Epoch [5/100], Loss: 0.4201\n",
      "Epoch [6/100], Loss: 1.2000\n",
      "Epoch [7/100], Loss: 0.9834\n",
      "Epoch [8/100], Loss: 0.2133\n",
      "Epoch [9/100], Loss: 1.0804\n",
      "Epoch [10/100], Loss: 0.3145\n",
      "Epoch [11/100], Loss: 0.1919\n",
      "Epoch [12/100], Loss: 0.6281\n",
      "Epoch [13/100], Loss: 0.8040\n",
      "Epoch [14/100], Loss: 0.3563\n",
      "Epoch [15/100], Loss: 0.3311\n",
      "Epoch [16/100], Loss: 0.3352\n",
      "Epoch [17/100], Loss: 0.2974\n",
      "Epoch [18/100], Loss: 0.2053\n",
      "Epoch [19/100], Loss: 1.1612\n",
      "Epoch [20/100], Loss: 1.0428\n",
      "Epoch [21/100], Loss: 0.6150\n",
      "Epoch [22/100], Loss: 0.4822\n",
      "Epoch [23/100], Loss: 0.7284\n",
      "Epoch [24/100], Loss: 0.2714\n",
      "Epoch [25/100], Loss: 1.2000\n",
      "Epoch [26/100], Loss: 0.4939\n",
      "Epoch [27/100], Loss: 0.5618\n",
      "Epoch [28/100], Loss: 0.5371\n",
      "Epoch [29/100], Loss: 0.3508\n",
      "Epoch [30/100], Loss: 0.4968\n",
      "Epoch [31/100], Loss: 0.6514\n",
      "Epoch [32/100], Loss: 0.4855\n",
      "Epoch [33/100], Loss: 0.6953\n",
      "Epoch [34/100], Loss: 0.4757\n",
      "Epoch [35/100], Loss: 0.4078\n",
      "Epoch [36/100], Loss: 0.4301\n",
      "Epoch [37/100], Loss: 0.2550\n",
      "Epoch [38/100], Loss: 0.3643\n",
      "Epoch [39/100], Loss: 0.4269\n",
      "Epoch [40/100], Loss: 0.9076\n",
      "Epoch [41/100], Loss: 0.4130\n",
      "Epoch [42/100], Loss: 0.3819\n",
      "Epoch [43/100], Loss: 0.6759\n",
      "Epoch [44/100], Loss: 0.3158\n",
      "Epoch [45/100], Loss: 0.4178\n",
      "Epoch [46/100], Loss: 0.3704\n",
      "Epoch [47/100], Loss: 0.3703\n",
      "Epoch [48/100], Loss: 0.4984\n",
      "Epoch [49/100], Loss: 0.2725\n",
      "Epoch [50/100], Loss: 0.4036\n",
      "Epoch [51/100], Loss: 1.0843\n",
      "Epoch [52/100], Loss: 0.7035\n",
      "Epoch [53/100], Loss: 0.7785\n",
      "Epoch [54/100], Loss: 0.9712\n",
      "Epoch [55/100], Loss: 1.1800\n",
      "Epoch [56/100], Loss: 0.3076\n",
      "Epoch [57/100], Loss: 0.3552\n",
      "Epoch [58/100], Loss: 0.6162\n",
      "Epoch [59/100], Loss: 0.6820\n",
      "Epoch [60/100], Loss: 0.5760\n",
      "Epoch [61/100], Loss: 0.3450\n",
      "Epoch [62/100], Loss: 0.2489\n",
      "Epoch [63/100], Loss: 0.3318\n",
      "Epoch [64/100], Loss: 0.7053\n",
      "Epoch [65/100], Loss: 0.2134\n",
      "Epoch [66/100], Loss: 0.6514\n",
      "Epoch [67/100], Loss: 0.7727\n",
      "Epoch [68/100], Loss: 0.5028\n",
      "Epoch [69/100], Loss: 0.5307\n",
      "Epoch [70/100], Loss: 0.6434\n",
      "Epoch [71/100], Loss: 0.3117\n",
      "Epoch [72/100], Loss: 0.5869\n",
      "Epoch [73/100], Loss: 0.1934\n",
      "Epoch [74/100], Loss: 0.4002\n",
      "Epoch [75/100], Loss: 0.4086\n",
      "Epoch [76/100], Loss: 0.3520\n",
      "Epoch [77/100], Loss: 0.6223\n",
      "Epoch [78/100], Loss: 0.7153\n",
      "Epoch [79/100], Loss: 0.1411\n",
      "Epoch [80/100], Loss: 0.3703\n",
      "Epoch [81/100], Loss: 0.5344\n",
      "Epoch [82/100], Loss: 0.3787\n",
      "Epoch [83/100], Loss: 0.7111\n",
      "Epoch [84/100], Loss: 0.2662\n",
      "Epoch [85/100], Loss: 0.2897\n",
      "Epoch [86/100], Loss: 0.8510\n",
      "Epoch [87/100], Loss: 0.3386\n",
      "Epoch [88/100], Loss: 0.1915\n",
      "Epoch [89/100], Loss: 0.6823\n",
      "Epoch [90/100], Loss: 0.4295\n",
      "Epoch [91/100], Loss: 0.4190\n",
      "Epoch [92/100], Loss: 0.4757\n",
      "Epoch [93/100], Loss: 0.3769\n",
      "Epoch [94/100], Loss: 0.2064\n",
      "Epoch [95/100], Loss: 1.0029\n",
      "Epoch [96/100], Loss: 0.8571\n",
      "Epoch [97/100], Loss: 0.5399\n",
      "Epoch [98/100], Loss: 0.5996\n",
      "Epoch [99/100], Loss: 0.8637\n",
      "Epoch [100/100], Loss: 0.3014\n",
      "Mean Squared Error: 0.5518\n",
      "le temps d'exécution est de : 25.38265371322632 secondes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "\n",
    "start_time1 = time.time()\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "X = california_housing.data\n",
    "y = california_housing.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "linear_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_reg.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse1 = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse1)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time1 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time1 = end_time1 - start_time1\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time1,\"secondes\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger la dataset California Housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Create an instance of XGBRegressor with linear base learner\n",
    "model = xgb.XGBRegressor(booster=\"gblinear\")\n",
    "\n",
    "# Entraîner le modèle de régression XGBoost\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Effectuer des prédictions sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time,\"secondes\")\n",
    "#keras\n",
    "import time\n",
    "\n",
    "start2=time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Chargement des données California Housing\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Normalisation des caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fractionnement des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(X_train.shape[1],), activation='linear'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "loss2 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error (MSE): {loss2}\")\n",
    "\n",
    "end2=time.time()\n",
    "execution_time2=end2 - start2\n",
    "print(\"le temps d'exécution est :\",execution_time2,\"secondes\")\n",
    "\n",
    "#tensorflow\n",
    "import time \n",
    "\n",
    "start3=time.time()\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create the TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_shape=(X_train.shape[1],))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse3)\n",
    "\n",
    "\n",
    "end3=time.time()\n",
    "execution_time3 =end3 - start3\n",
    "print(\"le temps d'exécution est de :  \",execution_time3,\"Seconds \") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#PyTorch\n",
    "import time as tm\n",
    "\n",
    "start=tm.time()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Charger les données California Housing\n",
    "data = fetch_california_housing()\n",
    "\n",
    "# Effectuer une normalisation des données\n",
    "scaler = StandardScaler()\n",
    "data.data = scaler.fit_transform(data.data)\n",
    "\n",
    "# Convertir les données en tenseurs PyTorch\n",
    "inputs = torch.tensor(data.data, dtype=torch.float32)\n",
    "targets = torch.tensor(data.target, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Définir une classe personnalisée pour le jeu de données\n",
    "class CaliforniaHousingDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "\n",
    "# Créer une instance du jeu de données\n",
    "dataset = CaliforniaHousingDataset(inputs, targets)\n",
    "\n",
    "# Définir la taille du lot (batch size) et créer le chargeur de données (data loader)\n",
    "dataloader = DataLoader(dataset, batch_size=42, shuffle=True)\n",
    "\n",
    "# Définir le modèle de régression linéaire\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Créer une instance du modèle\n",
    "input_size = inputs.shape[1]\n",
    "output_size = 1\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# Définir la fonction de perte (loss function) et l'optimiseur\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Entraînement du modèle\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_inputs, batch_targets in dataloader:\n",
    "        # Remise à zéro des gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prédiction du modèle\n",
    "        outputs = model(batch_inputs)\n",
    "\n",
    "        # Calcul de la perte\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "\n",
    "        # Rétropropagation et mise à jour des poids\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Affichage de la perte à chaque époque\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Évaluation du modèle\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_targets = model(inputs)\n",
    "    mse4= criterion(predicted_targets, targets)\n",
    "    print(f'Mean Squared Error: {mse4.item():.4f}')\n",
    "\n",
    "end=tm.time()\n",
    "execution_time4=end - start\n",
    "\n",
    "print(\"le temps d'exécution est de :\",execution_time4,\"secondes\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['scikit-learn', 'XGBOOST', 'keras', 'Tensorflow', 'pytorch']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "MSE_list = [mse1,mse, loss2, mse3, mse4]  \n",
    "execution_time_list = [execution_time1, execution_time, execution_time2, execution_time3, execution_time4]  \n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(x - width/2, MSE_list, width, label='MSE', color='#8AC847')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, execution_time_list, width, label='Execution Time', color='pink')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "\n",
    "fig.suptitle('MSE and Execution Time Comparison for Linear Regression Algorithm')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50759ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "250f06ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.6250016254543714\n",
      "Temps d'exécution: 2.1772568225860596 secondes\n",
      "Epoch 1/100\n",
      "258/258 [==============================] - 1s 2ms/step - loss: 0.9146\n",
      "Epoch 2/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.4258\n",
      "Epoch 3/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3778\n",
      "Epoch 4/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3614\n",
      "Epoch 5/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3535\n",
      "Epoch 6/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3353\n",
      "Epoch 7/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3269\n",
      "Epoch 8/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3174\n",
      "Epoch 9/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3121\n",
      "Epoch 10/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3080\n",
      "Epoch 11/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.3016\n",
      "Epoch 12/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.3004\n",
      "Epoch 13/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2973\n",
      "Epoch 14/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2936\n",
      "Epoch 15/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2889\n",
      "Epoch 16/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2879\n",
      "Epoch 17/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2829\n",
      "Epoch 18/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2815\n",
      "Epoch 19/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2812\n",
      "Epoch 20/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2774\n",
      "Epoch 21/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2768\n",
      "Epoch 22/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2726\n",
      "Epoch 23/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2725\n",
      "Epoch 24/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2688\n",
      "Epoch 25/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2683\n",
      "Epoch 26/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2688\n",
      "Epoch 27/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2670\n",
      "Epoch 28/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2685\n",
      "Epoch 29/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2642\n",
      "Epoch 30/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2635\n",
      "Epoch 31/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2617\n",
      "Epoch 32/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2602\n",
      "Epoch 33/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2588\n",
      "Epoch 34/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2595\n",
      "Epoch 35/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2561\n",
      "Epoch 36/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2566\n",
      "Epoch 37/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2575\n",
      "Epoch 38/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2550\n",
      "Epoch 39/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2563\n",
      "Epoch 40/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2538\n",
      "Epoch 41/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2517\n",
      "Epoch 42/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2503\n",
      "Epoch 43/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2496\n",
      "Epoch 44/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2501\n",
      "Epoch 45/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2460\n",
      "Epoch 46/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2445\n",
      "Epoch 47/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2460\n",
      "Epoch 48/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2438\n",
      "Epoch 49/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2453\n",
      "Epoch 50/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2474\n",
      "Epoch 51/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2451\n",
      "Epoch 52/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2423\n",
      "Epoch 53/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2408\n",
      "Epoch 54/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2407\n",
      "Epoch 55/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2386\n",
      "Epoch 56/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2389\n",
      "Epoch 57/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2399\n",
      "Epoch 58/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2371\n",
      "Epoch 59/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2381\n",
      "Epoch 60/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2367\n",
      "Epoch 61/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2350\n",
      "Epoch 62/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2357\n",
      "Epoch 63/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2331\n",
      "Epoch 64/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2353\n",
      "Epoch 65/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2336\n",
      "Epoch 66/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2349\n",
      "Epoch 67/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2342\n",
      "Epoch 68/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2306\n",
      "Epoch 69/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2311\n",
      "Epoch 70/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2419\n",
      "Epoch 71/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2308\n",
      "Epoch 72/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2310\n",
      "Epoch 73/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2305\n",
      "Epoch 74/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2285\n",
      "Epoch 75/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2292\n",
      "Epoch 76/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2266\n",
      "Epoch 77/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2282\n",
      "Epoch 78/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2284\n",
      "Epoch 79/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2279\n",
      "Epoch 80/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2267\n",
      "Epoch 81/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2264\n",
      "Epoch 82/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2255\n",
      "Epoch 83/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2251\n",
      "Epoch 84/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2241\n",
      "Epoch 85/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2226\n",
      "Epoch 86/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2224\n",
      "Epoch 87/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2229\n",
      "Epoch 88/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2220\n",
      "Epoch 89/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2205\n",
      "Epoch 90/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2224\n",
      "Epoch 91/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2230\n",
      "Epoch 92/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2210\n",
      "Epoch 93/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2221\n",
      "Epoch 94/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2193\n",
      "Epoch 95/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2195\n",
      "Epoch 96/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2184\n",
      "Epoch 97/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2181\n",
      "Epoch 98/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2190\n",
      "Epoch 99/100\n",
      "258/258 [==============================] - 0s 2ms/step - loss: 0.2173\n",
      "Epoch 100/100\n",
      "258/258 [==============================] - 0s 1ms/step - loss: 0.2194\n",
      "Mean Squared Error (MSE): 0.26690325140953064\n",
      "Temps d'exécution: 41.994258403778076 secondes\n",
      "Epoch 1/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 1.1314\n",
      "Epoch 2/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.5190\n",
      "Epoch 3/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.4404\n",
      "Epoch 4/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.4172\n",
      "Epoch 5/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.4060\n",
      "Epoch 6/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3998\n",
      "Epoch 7/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3933\n",
      "Epoch 8/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3897\n",
      "Epoch 9/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3824\n",
      "Epoch 10/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3793\n",
      "Epoch 11/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3748\n",
      "Epoch 12/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3728\n",
      "Epoch 13/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3698\n",
      "Epoch 14/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3671\n",
      "Epoch 15/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3621\n",
      "Epoch 16/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3618\n",
      "Epoch 17/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3603\n",
      "Epoch 18/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3528\n",
      "Epoch 19/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3543\n",
      "Epoch 20/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3516\n",
      "Epoch 21/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3537\n",
      "Epoch 22/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3447\n",
      "Epoch 23/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3438\n",
      "Epoch 24/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3423\n",
      "Epoch 25/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3398\n",
      "Epoch 26/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3382\n",
      "Epoch 27/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3396\n",
      "Epoch 28/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3371\n",
      "Epoch 29/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3343\n",
      "Epoch 30/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3355\n",
      "Epoch 31/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3303\n",
      "Epoch 32/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3325\n",
      "Epoch 33/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3286\n",
      "Epoch 34/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3287\n",
      "Epoch 35/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3277\n",
      "Epoch 36/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3250\n",
      "Epoch 37/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3333\n",
      "Epoch 38/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3238\n",
      "Epoch 39/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3243\n",
      "Epoch 40/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3233\n",
      "Epoch 41/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3201\n",
      "Epoch 42/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3228\n",
      "Epoch 43/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3235\n",
      "Epoch 44/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3201\n",
      "Epoch 45/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3168\n",
      "Epoch 46/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3170\n",
      "Epoch 47/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3177\n",
      "Epoch 48/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3199\n",
      "Epoch 49/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3175\n",
      "Epoch 50/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3148\n",
      "Epoch 51/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3133\n",
      "Epoch 52/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3143\n",
      "Epoch 53/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3131\n",
      "Epoch 54/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3114\n",
      "Epoch 55/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3313\n",
      "Epoch 56/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3114\n",
      "Epoch 57/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3104\n",
      "Epoch 58/100\n",
      "516/516 [==============================] - 1s 2ms/step - loss: 0.3096\n",
      "Epoch 59/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3088\n",
      "Epoch 60/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3126\n",
      "Epoch 61/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3090\n",
      "Epoch 62/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3088\n",
      "Epoch 63/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3122\n",
      "Epoch 64/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3135\n",
      "Epoch 65/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3078\n",
      "Epoch 66/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3072\n",
      "Epoch 67/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3094\n",
      "Epoch 68/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3064\n",
      "Epoch 69/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3064\n",
      "Epoch 70/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3076\n",
      "Epoch 71/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3059\n",
      "Epoch 72/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3063\n",
      "Epoch 73/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3051\n",
      "Epoch 74/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3066\n",
      "Epoch 75/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3229\n",
      "Epoch 76/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3035\n",
      "Epoch 77/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3031\n",
      "Epoch 78/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3032\n",
      "Epoch 79/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3016\n",
      "Epoch 80/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3069\n",
      "Epoch 81/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3036\n",
      "Epoch 82/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3164\n",
      "Epoch 83/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3049\n",
      "Epoch 84/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3013\n",
      "Epoch 85/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3042\n",
      "Epoch 86/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3035\n",
      "Epoch 87/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3032\n",
      "Epoch 88/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3029\n",
      "Epoch 89/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3029\n",
      "Epoch 90/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3054\n",
      "Epoch 91/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3053\n",
      "Epoch 92/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3014\n",
      "Epoch 93/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3004\n",
      "Epoch 94/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3029\n",
      "Epoch 95/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3092\n",
      "Epoch 96/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.2999\n",
      "Epoch 97/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.2993\n",
      "Epoch 98/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3065\n",
      "Epoch 99/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3039\n",
      "Epoch 100/100\n",
      "516/516 [==============================] - 1s 1ms/step - loss: 0.3048\n",
      "Mean Squared Error: 0.33767539262771606\n",
      "Le temps d'exécution est de  73.85649824142456 secondes\n",
      "WARNING:tensorflow:From c:\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilham\\AppData\\Local\\Temp\\ipykernel_12716\\2427168936.py:167: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  hidden = tf.layers.dense(X, n_neurons, activation=tf.nn.relu)\n",
      "C:\\Users\\ilham\\AppData\\Local\\Temp\\ipykernel_12716\\2427168936.py:170: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  output = tf.layers.dense(hidden, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 0.4096282124519348\n",
      "Epoch 1: Train Loss = 0.38184791803359985\n",
      "Epoch 2: Train Loss = 0.3635615110397339\n",
      "Epoch 3: Train Loss = 0.34285634756088257\n",
      "Epoch 4: Train Loss = 0.34655189514160156\n",
      "Epoch 5: Train Loss = 0.3428822457790375\n",
      "Epoch 6: Train Loss = 0.3312874138355255\n",
      "Epoch 7: Train Loss = 0.33843764662742615\n",
      "Epoch 8: Train Loss = 0.32622143626213074\n",
      "Epoch 9: Train Loss = 0.3246135711669922\n",
      "Epoch 10: Train Loss = 0.3202117085456848\n",
      "Epoch 11: Train Loss = 0.31919121742248535\n",
      "Epoch 12: Train Loss = 0.3197628855705261\n",
      "Epoch 13: Train Loss = 0.3143361508846283\n",
      "Epoch 14: Train Loss = 0.3175846338272095\n",
      "Epoch 15: Train Loss = 0.3143915832042694\n",
      "Epoch 16: Train Loss = 0.3158685266971588\n",
      "Epoch 17: Train Loss = 0.30958521366119385\n",
      "Epoch 18: Train Loss = 0.31052127480506897\n",
      "Epoch 19: Train Loss = 0.31109046936035156\n",
      "Epoch 20: Train Loss = 0.3106048107147217\n",
      "Epoch 21: Train Loss = 0.30760130286216736\n",
      "Epoch 22: Train Loss = 0.31128090620040894\n",
      "Epoch 23: Train Loss = 0.3029082417488098\n",
      "Epoch 24: Train Loss = 0.30422767996788025\n",
      "Epoch 25: Train Loss = 0.30837830901145935\n",
      "Epoch 26: Train Loss = 0.3067231774330139\n",
      "Epoch 27: Train Loss = 0.2979798913002014\n",
      "Epoch 28: Train Loss = 0.2998243272304535\n",
      "Epoch 29: Train Loss = 0.3033405840396881\n",
      "Epoch 30: Train Loss = 0.3036336898803711\n",
      "Epoch 31: Train Loss = 0.3033924102783203\n",
      "Epoch 32: Train Loss = 0.3027084469795227\n",
      "Epoch 33: Train Loss = 0.29895883798599243\n",
      "Epoch 34: Train Loss = 0.2974960505962372\n",
      "Epoch 35: Train Loss = 0.2977786362171173\n",
      "Epoch 36: Train Loss = 0.31648266315460205\n",
      "Epoch 37: Train Loss = 0.29969730973243713\n",
      "Epoch 38: Train Loss = 0.29916346073150635\n",
      "Epoch 39: Train Loss = 0.29732218384742737\n",
      "Epoch 40: Train Loss = 0.29579973220825195\n",
      "Epoch 41: Train Loss = 0.299140602350235\n",
      "Epoch 42: Train Loss = 0.2995077669620514\n",
      "Epoch 43: Train Loss = 0.31168943643569946\n",
      "Epoch 44: Train Loss = 0.297039270401001\n",
      "Epoch 45: Train Loss = 0.2976229190826416\n",
      "Epoch 46: Train Loss = 0.2986515164375305\n",
      "Epoch 47: Train Loss = 0.30675479769706726\n",
      "Epoch 48: Train Loss = 0.3154844641685486\n",
      "Epoch 49: Train Loss = 0.30381691455841064\n",
      "Epoch 50: Train Loss = 0.2985548973083496\n",
      "Epoch 51: Train Loss = 0.2965877950191498\n",
      "Epoch 52: Train Loss = 0.29687410593032837\n",
      "Epoch 53: Train Loss = 0.29438042640686035\n",
      "Epoch 54: Train Loss = 0.2954324781894684\n",
      "Epoch 55: Train Loss = 0.29675689339637756\n",
      "Epoch 56: Train Loss = 0.2952542006969452\n",
      "Epoch 57: Train Loss = 0.29397472739219666\n",
      "Epoch 58: Train Loss = 0.2947597801685333\n",
      "Epoch 59: Train Loss = 0.2964443266391754\n",
      "Epoch 60: Train Loss = 0.2924557626247406\n",
      "Epoch 61: Train Loss = 0.291854590177536\n",
      "Epoch 62: Train Loss = 0.29316434264183044\n",
      "Epoch 63: Train Loss = 0.29379335045814514\n",
      "Epoch 64: Train Loss = 0.2920517325401306\n",
      "Epoch 65: Train Loss = 0.2899821698665619\n",
      "Epoch 66: Train Loss = 0.2902534008026123\n",
      "Epoch 67: Train Loss = 0.2883983254432678\n",
      "Epoch 68: Train Loss = 0.2900438606739044\n",
      "Epoch 69: Train Loss = 0.28917962312698364\n",
      "Epoch 70: Train Loss = 0.2880173325538635\n",
      "Epoch 71: Train Loss = 0.2927681505680084\n",
      "Epoch 72: Train Loss = 0.2894042432308197\n",
      "Epoch 73: Train Loss = 0.2887425124645233\n",
      "Epoch 74: Train Loss = 0.2872263789176941\n",
      "Epoch 75: Train Loss = 0.29098746180534363\n",
      "Epoch 76: Train Loss = 0.2867681682109833\n",
      "Epoch 77: Train Loss = 0.2867980897426605\n",
      "Epoch 78: Train Loss = 0.2882973551750183\n",
      "Epoch 79: Train Loss = 0.28992167115211487\n",
      "Epoch 80: Train Loss = 0.29131895303726196\n",
      "Epoch 81: Train Loss = 0.28830453753471375\n",
      "Epoch 82: Train Loss = 0.2913023829460144\n",
      "Epoch 83: Train Loss = 0.2881503999233246\n",
      "Epoch 84: Train Loss = 0.28835010528564453\n",
      "Epoch 85: Train Loss = 0.286997526884079\n",
      "Epoch 86: Train Loss = 0.28753554821014404\n",
      "Epoch 87: Train Loss = 0.28756898641586304\n",
      "Epoch 88: Train Loss = 0.2890622317790985\n",
      "Epoch 89: Train Loss = 0.2894064784049988\n",
      "Epoch 90: Train Loss = 0.2879430651664734\n",
      "Epoch 91: Train Loss = 0.28787973523139954\n",
      "Epoch 92: Train Loss = 0.28644630312919617\n",
      "Epoch 93: Train Loss = 0.2944301664829254\n",
      "Epoch 94: Train Loss = 0.28767070174217224\n",
      "Epoch 95: Train Loss = 0.28831619024276733\n",
      "Epoch 96: Train Loss = 0.28731074929237366\n",
      "Epoch 97: Train Loss = 0.2872491180896759\n",
      "Epoch 98: Train Loss = 0.288296639919281\n",
      "Epoch 99: Train Loss = 0.28618839383125305\n",
      "Test Loss = 0.3141017556190491\n",
      "le temps d'exécution est de  28.01390504837036 secondes\n",
      "MSE: 0.60\n",
      "Temps d'exécution: 14.218035459518433 secondes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_time1=time.time()\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.neural_network import MLPRegressor #Multi-Layer Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Train the model\n",
    "model = MLPRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Calculate the mean squared error of the model\n",
    "mse1 = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse1)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time1 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time1 = end_time1 - start_time1\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time1,\"secondes\")\n",
    "\n",
    "import time \n",
    "\n",
    "start_time2=time.time()\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Chargement des données California Housing\n",
    "data = fetch_california_housing()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "# Normalisation des caractéristiques\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Fractionnement des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Création du modèle\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compilation du modèle\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "mse2 = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Mean Squared Error (MSE): {mse2}\")\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time2 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time2 = end_time2 - start_time2\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time2,\"secondes\")\n",
    "\n",
    "import time as tm\n",
    "\n",
    "start3=tm.time()\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "mse3 = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(\"Mean Squared Error:\", mse3)\n",
    "\n",
    "end3=tm.time()\n",
    "\n",
    "execution_time3=end3 - start3\n",
    "\n",
    "print(\"Le temps d'exécution est de \",execution_time3,\"secondes\")\n",
    "\n",
    "import time as tm\n",
    "\n",
    "start4 = tm.time()\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tf.disable_v2_behavior()  # Enable compatibility with TensorFlow 1.x\n",
    "\n",
    "# Load the California housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target.reshape(-1, 1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the ANN model\n",
    "n_features = X_train_scaled.shape[1]\n",
    "n_neurons = 30\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "# Define placeholders for inputs and targets\n",
    "X = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, n_features), name='X')\n",
    "y = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 1), name='y')\n",
    "\n",
    "# Define the hidden layer\n",
    "hidden = tf.layers.dense(X, n_neurons, activation=tf.nn.relu)\n",
    "\n",
    "# Define the output layer\n",
    "output = tf.layers.dense(hidden, 1)\n",
    "\n",
    "# Define the loss function\n",
    "loss = tf.reduce_mean(tf.square(output - y))\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "# Create a session and initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(n_epochs):\n",
    "    for iteration in range(X_train_scaled.shape[0] // batch_size):\n",
    "        X_batch = X_train_scaled[iteration*batch_size:(iteration+1)*batch_size]\n",
    "        y_batch = y_train[iteration*batch_size:(iteration+1)*batch_size]\n",
    "        sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "    if epoch % 1== 0:\n",
    "        train_loss = sess.run(loss, feed_dict={X: X_train_scaled, y: y_train})\n",
    "        print(f'Epoch {epoch}: Train Loss = {train_loss}')\n",
    "\n",
    "# Evaluate the model\n",
    "mse4 = sess.run(loss, feed_dict={X: X_test_scaled, y: y_test})\n",
    "print(f'Test Loss = {mse4}')\n",
    "\n",
    "end4= tm.time()\n",
    "\n",
    "execution_time4 =end4-start4\n",
    "print(\"le temps d'exécution est de \",execution_time4,\"secondes\")\n",
    "\n",
    "\n",
    "import time \n",
    "\n",
    "start_time5=time.time()\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "\n",
    "# Read data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# train-test split for model evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, shuffle=True)\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 24),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(24, 12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(12, 6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(6, 1)\n",
    ")\n",
    "\n",
    "# loss function and optimizer\n",
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "n_epochs = 100   # number of epochs to run\n",
    "batch_size = 32 # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = np.inf   # init to infinity\n",
    "best_weights = None\n",
    "history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "        bar.set_description(f\"Epoch {epoch}\")\n",
    "        for start in bar:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "            # forward pass\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            \n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "    # evaluate accuracy at end of each epoch\n",
    "    model.eval()\n",
    "    y_pred = model(X_test)\n",
    "    mse = loss_fn(y_pred, y_test)\n",
    "    mse = float(mse)\n",
    "    history.append(mse)\n",
    "    if mse < best_mse:\n",
    "        mse5 = mse\n",
    "        best_weights = copy.deepcopy(model.state_dict())\n",
    "\n",
    "# restore model and return best accuracy\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % mse5)\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time5 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time5 = end_time5 - start_time5\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time5,\"secondes\")\n",
    "\n",
    "\n",
    "\n",
    "# Create lists to store accuracy and execution time\n",
    "accuracy_list = [mse1, mse2, mse3, mse4, mse5 ]\n",
    "execution_time_list = [execution_time1, execution_time2, execution_time3,execution_time4,execution_time5]\n",
    "\n",
    "# Plotting the bar graph\n",
    "labels = ['scikit-learn', 'Keras', 'TensorFlow (keras API)','TensorFlow','Pytorch']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(x - width/2, accuracy_list, width, label=' MSE', color= '#8AC847')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, execution_time_list, width, label='Execution Time', color='pink')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "\n",
    "fig.suptitle('Accuracy and Execution Time Comparison for Neural Network algorithm (Regression)')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22196b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Trees Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0a73ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python311\\Lib\\site-packages\\lightgbm\\engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000676 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "Mean Squared Error: 0.22458122312911416\n",
      "Temps d'exécution: 0.0956718921661377 secondes\n",
      "Mean Squared Error: 0.6062225034373102\n",
      "Execution Time: 0.42936253547668457 seconds\n",
      "Mean Squared Error: 0.22458289556216388\n",
      "le temps d'excution est de  0.6026735305786133\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Enregistrer le temps de départ\n",
    "start_time1 = time.time()\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Charger la dataset California Housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of LGBMRegressor\n",
    "gbm = lgb.LGBMRegressor(boosting_type = 'gbdt',objective = 'regression',learning_rate= 0.1,verbose= 0,max_depth= 6, num_iterations= 100) # approximate linear regression using LightGBM with decision trees as base learners\n",
    "\n",
    "# Entraîner le modèle\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Effectuer des prédictions sur l'ensemble de test\n",
    "y_pred = gbm.predict(X_test)\n",
    "\n",
    "# Calculer l'erreur quadratique moyenne (MSE)\n",
    "mse1 = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse1)\n",
    "\n",
    "# Enregistrer le temps d'arrivée\n",
    "end_time1 = time.time()\n",
    "\n",
    "# Calculer la durée totale d'exécution\n",
    "execution_time1 = end_time1 - start_time1\n",
    "\n",
    "# Afficher le temps d'exécution\n",
    "print(\"Temps d'exécution:\", execution_time1,\"secondes\")\n",
    "import time\n",
    "\n",
    "# Register the start time\n",
    "start_time2 = time.time()\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an instance of CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=100, learning_rate=0.01,depth=6)\n",
    "\n",
    "# Train the regression model\n",
    "model.fit(X_train, y_train, verbose=False)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error (MSE)\n",
    "mse2 = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse2)\n",
    "\n",
    "# Register the end time\n",
    "end_time2 = time.time()\n",
    "\n",
    "# Calculate the total execution time\n",
    "execution_time2 = end_time2 - start_time2\n",
    "\n",
    "# Print the execution time\n",
    "print(\"Execution Time:\", execution_time2, \"seconds\")\n",
    "\n",
    "import time\n",
    "\n",
    "start_time3=time.time()\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Charger la dataset California Housing\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer un modèle de régression XGBoost\n",
    "model = xgb.XGBRegressor(n_estimators=100)\n",
    "\n",
    "# Entraîner le modèle de régression XGBoost\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# es prédictions sur la partie  de test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse3 = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse3)\n",
    "\n",
    "end_time3=time.time()\n",
    "\n",
    "execution_time3=end_time3-start_time3\n",
    "\n",
    "print(\"le temps d'excution est de \",execution_time3)\n",
    "\n",
    "# Create lists to store accuracy and execution time\n",
    "accuracy_list = [mse1, mse2, mse3]\n",
    "execution_time_list = [execution_time1, execution_time2, execution_time3]\n",
    "\n",
    "# Plotting the bar graph\n",
    "labels = ['LightGBM', 'CatBoost','XGBoost']\n",
    "x = np.arange(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(x - width/2, accuracy_list, width, label='MSE', color= '#8AC847')\n",
    "ax1.set_ylabel('MSE')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(x + width/2, execution_time_list, width, label='Execution Time', color='pink')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "\n",
    "fig.suptitle('Accuracy and Execution Time Comparison for regression with Boosted Trees')\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
